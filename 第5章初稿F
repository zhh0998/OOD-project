# 通过异质图因果解耦与生成式原型学习的无监督文本OOD检测框架





## 第一部分：引言与问题定义





### 1.1 研究背景与动机



随着深度学习技术的飞速发展，尤其是大语言模型（Large Language Models, LLMs）的崛起，人工智能系统正以前所未有的深度和广度渗透到社会生活的各个层面。然而，这些模型在部署于真实、开放的世界中时，面临一个根本性的挑战：它们必须能够应对与训练数据分布显著不同的输入，即分布外（Out-of-Distribution, OOD）样本。无法有效识别和处理OOD输入，可能导致模型产生不可靠甚至危险的预测，这在自动驾驶、医疗诊断、金融风控等安全攸关领域是不可接受的 。因此，研究和开发高效的OOD检测技术，对于保障AI系统的安全、可靠和可信至关重要。

本研究聚焦于句子级别的文本OOD检测。与图像等其他模态相比，文本的OOD表现形式更为复杂和微妙。它不仅体现在主题或实体层面的语义偏移（Semantic Shift），例如从讨论“体育”的分布内（In-Distribution, ID）数据转向讨论“政治”的OOD数据；更体现在风格、句法结构、情感倾向、甚至是逻辑关系的变异（Stylistic/Syntactic/Logical Shift）。一个句子可能在主题上与ID数据高度相关，但其论证方式或所蕴含的深层逻辑却可能是全新的、前所未见的。这种细粒度的OOD特性要求检测算法具备对语言深层次、多维度的理解能力。

传统的OOD检测方法通常将每个文本样本视为独立的实例进行分析，这忽略了文本数据之间固有的、丰富的关联性。为了克服这一局限，引入图结构（Graph Structure）来显式地建模句子间的复杂关系，成为一个极具前景的研究方向。通过将每个句子视为图中的一个节点，句子间的关系（如语义相似性、逻辑承接、观点对立等）则可以被建模为边。这种表示方法使得我们能够利用图神经网络（Graph Neural Networks, GNNs）强大的结构推理能力。图OOD（Graph Out-of-Distribution, GOOD）检测作为一个新兴领域，正是致力于解决图数据中的OOD问题，其核心优势在于能够同时考虑节点的自身属性和其所处的拓扑环境，从而对节点的“常规性”做出更全面的判断 2。将GOOD的理论框架应用于文本领域，为解决句子级别的OOD检测难题提供了全新的视角和强大的工具。

然而，将这一理念付诸实践并非易事。它引出了一系列深刻的科学问题，例如：如何为文本构建一个高质量的、任务导向的图？如何将LLM无与伦比的语义理解能力与GNN卓越的结构推理能力进行深度融合？以及，如何在一个完全无监督的设定下，实现超越现有方法的检测性能？这些问题构成了本研究的核心动机。本论文旨在探索一种全新的范式，它不仅判断一个句子“是否”是OOD，更试图理解它“为什么”是OOD。通过引入因果推断的思想，我们期望解耦出文本中稳定的、决定其核心内容的“因果特征”与易变的、与分布偏移更相关的“混淆特征”。这种从现象判断到根源探究的范式转变，有望构建出更鲁棒、更可解释、也更值得信赖的文本OOD检测系统，这是迈向通用人工智能安全部署的关键一步。



### 1.2 核心挑战与问题定义



在将上述研究动机转化为一个具体、可行的博士研究课题时，我们必须直面并解决以下三大核心挑战：

1. 挑战1：无监督设定下的性能瓶颈 (Performance Bottleneck in Unsupervised Setting)

   在绝大多数真实应用场景中，我们无法预知未来可能遇到的所有OOD类型，因此也无法获取带标签的OOD数据用于监督训练。这要求我们的算法必须是完全无监督的。然而，现有的无监督OOD检测方法往往难以精确地刻画出高维、复杂ID数据分布的边界。一个常见且有效提升性能的策略是“OOD曝光”（OOD Exposure），即利用一个代理OOD数据集来帮助模型学习ID与非ID的差异。但高质量的代理OOD数据集难以获取，且一个有限的数据集无法代表无限的OOD空间，这限制了该策略的泛化能力和现实可用性 5。因此，如何在不依赖任何外部OOD样本的情况下，仅从ID数据中学习到足够强大的判别能力，是本研究的首要挑战。

2. 挑战2：LLM与GNN的深度融合难题 (The Dilemma of Deep LLM-GNN Fusion)

   LLM和GNN分别在语义理解和结构推理上拥有巨大优势，它们的结合为文本OOD检测带来了曙光。然而，如何实现1+1 > 2的深度融合而非简单的能力叠加，是一个悬而未决的难题。现有融合范式主要分为两类：一类是“GNN为中心”（GNN-centric），将LLM用作一个强大的特征提取器，把句子编码为固定维度的向量后输入GNN 8。这种方式虽然简单，但LLM中蕴含的丰富、动态的语义信息在被压缩为静态向量时不可避免地会发生损失。另一类是“LLM为中心”（LLM-centric），将图结构信息线性化为文本序列，直接输入给LLM进行处理 9。这种方式能更好地利用LLM的推理能力，但往往难以有效捕捉复杂的图拓扑结构，且计算成本极高。因此，设计一种新颖的架构，让LLM和GNN能够在多个层面进行信息交互、协同推理，是实现技术突破的关键。

3. 挑战3：高质量异质图的构建与推理 (Construction and Inference on High-Quality Heterophilic Graphs)

   GNN的经典设计基于“同质性”（Homophily）假设，即相互连接的节点倾向于具有相似的特征或标签。然而，在真实的文本网络中，“异质性”（Heterophily）现象普遍存在 10。例如，在学术引用网络中，一篇论文可能会引用一篇提出相反观点的论文来进行驳斥；在社交媒体上，观点对立的帖子之间常常发生直接的回复和互动。对于OOD检测任务而言，这种异质性关系尤其重要——一个新颖的OOD句子，可能正因为它提出了与众不同的观点，而与周围的ID句子形成了强烈的异质连接。因此，如何为我们的句子数据集构建一个能有效捕捉并利用异质性关系的高质量图，并设计出能在此类图上高效推理的GNN模型，是当前图学习领域的前沿热点，也是我们方案成功的基石 12。

基于以上分析，我们将本研究的**问题形式化定义**如下：

给定一个仅包含分布内（ID）样本的句子集合 Din={s1,s2,...,sN}，其中每个 si 是一个文本句子。我们的目标是设计一个无监督学习框架，该框架能够学习一个打分函数 f:S→R，其中 S 是所有可能的句子的空间。对于任意一个测试句子 stest∈S，函数 f(stest) 输出一个OOD分数。存在一个阈值 δ，使得：

label(stest)={IDOODif f(stest)≤δif f(stest)>δ

该框架的设计必须在无监督条件下完成，即在训练过程中不能访问任何来自 Dout 的样本。



### 1.3 本文贡献



为应对上述挑战，本研究拟提出一个创新的无监督文本OOD检测框架，其主要贡献可概括为以下四点：

1. **方法论创新：** 提出一个全新的、融合了因果解耦、生成式原型学习和异质图推理的无监督文本OOD检测框架。该框架旨在从“是什么”OOD提升到“为什么”OOD的认知层面，增强了检测的鲁棒性和可解释性。
2. **LLM驱动的异质图构建：** 设计一种新颖的、由LLM驱动的、面向OOD检测任务的文本图构建策略。该策略超越了传统的语义相似性建图，专注于挖掘句子间的语义对比、逻辑转折等“语义张力”关系，从而自然地构建一个富含异质性信息的图结构。
3. **因果解耦的表征学习：** 首次将因果推断中的解耦思想引入到文本图OOD检测任务中。通过分离句子的“稳定”核心语义（因果特征）与“易变”的风格、句法伪影（混淆特征），使得模型能够更精准地定位OOD的根源，从而提升在复杂分布偏移下的检测性能。
4. **全面的实验验证：** 将在我们指定的、具有挑战性的数据集上进行详尽的实验。这不仅包括与当前最先进方法的性能对比，还包括深入的消融研究，以验证所提框架中每一个创新模块的独立贡献和必要性。



## 第二部分：相关工作与前沿分析



本部分将系统梳理与本研究相关的三个核心领域：无监督OOD检测方法论、LLM与GNN的融合架构，以及文本图构建与异质性处理。通过对现有工作的深入分析，我们将明确本研究的技术定位与创新切入点。



### 2.1 无监督OOD检测方法论



无监督OOD检测旨在仅利用ID数据来学习一个能够识别未知异常的检测器。根据其核心原理，现有方法大致可分为以下几类：

- 基于重构的方法 (Reconstruction-based Methods):

  这类方法的核心思想是训练一个模型（通常是自编码器Autoencoder或其变体）来学习如何精确地重构ID数据。其基本假设是，模型在ID数据上训练后，对ID样本的重构能力强，重构误差小；而对于未见过的OOD样本，由于其模式与ID数据不同，模型难以有效重构，从而产生较大的重构误差。这个重构误差便可作为OOD分数 14。这类方法实现简单、直观，但其有效性高度依赖于重构误差能否真实反映样本与ID分布的偏离程度。在复杂的文本语义空间中，两个句子可能在语义上有巨大差异，但它们的浅层表示（如通过一个简单的AE）可能仍然相似，导致重构误差不足以区分细微的OOD样本。

- 基于距离/原型的方法 (Distance/Prototype-based Methods):

  这类方法试图在深度特征空间中为ID数据找到一个或多个紧凑的表示中心，即“原型”（Prototype）。检测时，通过计算测试样本的特征向量与这些原型的距离来进行OOD判断。早期的工作通常为每个类别或整个ID数据学习一个单一的原型，但这显然忽略了ID数据内部可能存在的复杂多模态分布 15。

  

  一个里程碑式的工作是PALM (ICLR 2024) 15，它提出“原型混合”（Mixture of Prototypes）的概念，允许用多个原型来共同描述一个类别的分布。通过这种方式，PALM能够更好地捕捉ID数据的内部多样性，学习到更紧凑的类内簇和更清晰的类间边界，从而显著提升OOD检测性能。PALM的思想对于处理语义丰富、内涵多样的文本数据极具启发性，因为它允许我们将一个宽泛的ID主题（如“体育新闻”）建模为多个子主题（如“篮球”、“足球”、“网球”）的原型集合。

- 基于能量的模型 (Energy-based Models, EBMs):

  EBMs将一个能量函数与神经网络相结合，通过训练使得ID样本对应于能量函数的低谷区域（低能量值），而OOD样本则被推向高能量区域。在测试时，样本的能量值直接作为其OOD分数 17。EBMs在理论上非常优雅，因为它直接对数据的概率密度进行建模。然而，在图结构数据上应用EBMs面临独特的挑战，尤其是在异质图中，如何定义和传播节点的能量是一个复杂问题。近期在ICML 2024上发表的

  **DeGEM** 13 提出了一种解耦方案，将图编码器的学习与能量函数的学习分离开，有效解决了MCMC采样在图结构上的困难，并在同质和异质图上都取得了优异的性能，代表了EBM在图OOD检测领域的一个重要进展。

- 基于生成的方法 (Generative Methods):

  这是近年来最前沿、也最具潜力的研究方向。这类方法的核心在于，既然我们缺少真实的OOD样本，那么能否利用生成模型来创造出高质量的“伪OOD样本”用于训练？

  - **扩散模型驱动：** **DiffPath (NeurIPS 2024)** 18 的研究表明，一个在大规模数据上预训练好的无条件扩散模型，其本身就蕴含了对数据分布的深刻理解。通过测量一个样本在扩散模型“去噪路径”上的某些几何特性（如路径的曲率），就可以在多个不同的OOD检测任务上取得有竞争力的结果，而无需为每个任务单独训练模型。这展示了大型生成模型在OOD检测中的巨大潜力。
  - **对抗生成驱动：** 即将发表在ICLR 2025的**GOLD** 6 框架提出了一个极具创新性的“隐式对抗学习”流程。GOLD不依赖任何外部OOD数据或预训练的大型生成模型，它在训练过程中引入一个轻量级的生成器和一个判别器（即OOD检测器）。生成器的任务是模仿GNN从ID数据中提取的特征嵌入，而判别器的任务则是区分真实的ID嵌入和生成器产生的“伪造”嵌入。通过一种交替优化的对抗训练，判别器被迫学习一个能量函数，使得ID嵌入能量低，而伪造嵌入能量高。同时，生成器为了“欺骗”判别器，会倾向于在ID数据分布的“边缘”或“低密度”区域生成样本。这些被判别器赋予高能量的生成样本，就自然成为了高质量的、紧贴ID分布边界的“硬”伪OOD样本，极大地增强了OOD检测器的训练效果。GOLD的思路巧妙地解决了无监督OOD检测的核心痛点，是我们最终方案的重要灵感来源。

为了更清晰地对比这些前沿方法，下表总结了我们重点参考的三种范式：

| 方法名称                 | 核心思想                                                     | 优点                                                         | 缺点                                                         | 对我们方案的启发                                             |
| ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **PALM (ICLR 2024)** 16  | **多原型学习**：使用原型混合（Mixture of Prototypes）为每个ID类别建模，以捕捉数据内部的多样性。 | 能够学习到更紧凑、更真实的ID数据表示，有效处理多模态分布。   | 仍属于基于距离的方法，其OOD分数计算较为简单，未考虑生成“边界”样本来增强模型。 | 启发我们在因果特征空间中，也应使用多原型来刻画ID句子的多模态语义分布。 |
| **DeGEM (ICML 2024)** 13 | **解耦能量模型**：将图编码器的学习与能量函数的学习解耦，解决了能量模型在图上训练困难的问题。 | 在同质和异质图上均表现出色，特别是在处理异质性方面有独到设计。 | 模型结构相对复杂，包含多个协同工作的组件，实现和调优难度较大。 | 启发我们OOD分数的计算可以是一个独立模块，并且需要特别设计以适应异质图结构。 |
| **GOLD (ICLR 2025)** 6   | **隐式对抗生成**：通过对抗训练，在不依赖外部数据的情况下，生成紧贴ID分布边界的“伪OOD”样本来增强检测器。 | 完全无监督，不依赖任何外部OOD数据或大型预训练生成模型，实用性强，思想新颖。 | 其能量函数的定义主要在节点嵌入空间，可能未充分利用图的结构信息，且对于复杂语义的能量建模可能不够精细。 | 核心启发：我们可以借鉴其对抗生成思想，在我们的原型定义的因果空间中生成伪OOD样本，以校准和强化OOD检测器。 |



### 2.2 LLM与GNN融合架构



将LLM的语义能力与GNN的结构能力相结合，是近年来NLP和图学习领域交叉的热点。其融合架构经历了从简单到复杂的演进过程，体现了研究者对两者能力边界和协同方式的不断深化理解。

- **GNN-centric (GNN为中心):** 这是最直接的融合方式。LLM（如BERT）首先被用作一个强大的文本编码器，将每个节点的文本属性（在我们的场景中是每个句子）转换成一个固定维度的向量。然后，这些向量作为初始节点特征输入到一个标准的GNN中，由GNN负责后续的图结构信息聚合和推理 8。这种方法的优点是架构清晰，易于实现，可以方便地将任何预训练语言模型与任何GNN结合。但其主要缺点是“信息压缩损失”：LLM生成的丰富上下文表示被强制压缩成一个静态的、固定长度的向量，这会丢失大量的细粒度语义信息，限制了模型性能的上限。
- **LLM-centric (LLM为中心):** 这种范式反其道而行之，试图将GNN的任务也交给LLM来完成。具体做法通常是“图结构序列化”（Graph Structure Serialization），即将一个节点的邻域结构（甚至整个图）通过某种预定义的格式（如模板、邻接表文本）转换成一段自然语言描述，然后与节点自身的文本内容拼接在一起，作为输入喂给LLM 9。这种方法的最大优势是能够充分利用LLM强大的上下文理解和推理能力，理论上可以捕捉非常复杂的依赖关系。然而，它的弊端也同样明显：首先，将非欧几里得的图结构强行线性化为序列文本，本身就会造成结构信息的损失和扭曲；其次，随着邻域大小的增加，输入序列的长度会急剧膨胀，带来巨大的计算开销和对LLM上下文窗口长度的挑战。
- **深度融合与协同进化 (Deep Fusion and Co-evolution):** 为了克服上述两种范式的局限，最新的研究趋势是探索更深层次、更动态的融合机制，让LLM和GNN不再是简单的上下游关系，而是协同工作的伙伴。
  - **结构感知Transformer：** 一篇名为**GL-Fusion**的预印本论文 9 提出了一种引人注目的架构，它将GNN的消息传递机制直接嵌入到LLM的Transformer层中。具体来说，在自注意力计算之后，增加一个“图注意力”模块，使得每个token在更新自身表示时，不仅能关注到序列中的其他token，还能关注到其在图结构中的邻居节点。这种设计实现了文本语义信息和图结构信息的同步、逐层处理。
  - **双向交叉注意力：** **GMLM (arXiv 2024)** 20 则设计了一个精巧的双向交叉注意力模块。它首先分别用GNN和LLM得到图结构表示和文本语义表示，然后通过交叉注意力机制，让图表示去“查询”文本表示中的相关信息，同时让文本表示也去“查询”图表示中的结构信息，相互增强，最终生成一个信息更丰富的联合表示。
  - **LLM赋能的GNN：** **LLM4HeG (arXiv 2024)** 8 的工作则展示了LLM在图学习中扮演新角色的潜力。它利用一个经过微调的LLM来分析图中每条边的两个端点节点的文本内容，并基于其语义关系来预测这条边的权重。这个由LLM生成的动态边权重，随后被用于指导一个GNN在异质图上进行更精细化的信息聚合。

这一系列研究的演进轨迹清晰地表明，LLM在融合框架中的角色正在发生深刻的转变——从一个被动的、提供初始特征的“编码器”，逐渐演变为一个主动的、参与图推理过程的“结构化推理器”。早期的工作仅仅利用LLM的编码能力，而最新的研究则开始探索让LLM去理解图的内在逻辑，例如判断邻居的重要性（注意力机制）、评估关系的强弱（边权重学习），甚至是决定信息处理的优先级（动态节点选择 20）。我们的方案设计必须顺应这一前沿趋势。LLM不应仅仅在输入端提供一次性的特征嵌入，更应该在图的构建、权重的赋予以及后续的推理等多个环节，扮演“智能顾问”和“结构推理引擎”的角色。这正是我们提出的“LLM驱动的异质图构建”策略的核心思想。



### 2.3 文本图构建与异质性处理



为句子集合构建一个高质量的图是整个方法的基础。这个图的质量直接决定了GNN能否学到有意义的表示。

- LLM驱动的图构建 (LLM-driven Graph Construction):

  利用LLM自动化地从非结构化文本中构建知识图谱（Knowledge Graph Construction, KGC）已成为一个热门研究方向 21。传统方法是设计复杂的Prompt，让LLM从文本中抽取出“主-谓-宾”（Subject-Predicate-Object）三元组，形成知识图谱的节点和边。然而，对于我们的OOD检测任务，我们需要的不是一个通用的、事实性的知识图谱，而是一个能够最大化地区分ID和OOD样本的“任务导向图”。

- 异质性（Heterophily）的挑战与机遇:

  经典GNN模型在消息传递时，本质上是在对邻居节点的特征进行加权平均，这隐含了一个“同质性”的假设，即邻居和中心节点是相似的，因此聚合邻居信息可以增强中心节点的表示并使其平滑。然而，正如前文所述，文本网络中充满了异质性 10。在一个充满对立、驳斥、转折关系的异质图上，盲目地聚合邻居特征反而会“污染”中心节点的表示，导致性能下降。

  

  然而，对于OOD检测任务，这种看似“麻烦”的异质性，实际上蕴含着宝贵的信息。传统观点将异质性视为GNN性能的“挑战”或“问题” 10，需要被“克服”。但如果我们转换视角，就会发现异质性本身就是一种强有力的OOD信号。一个常规的ID句子，其周围的邻居很可能大多是语义相似、观点一致的“同质”邻居。而一个新颖的OOD句子，即便其讨论的主题与ID数据相关，但由于它可能引入了全新的视角、颠覆性的结论或不寻常的逻辑，它与周围的ID句子之间就极有可能形成强烈的“异质”关系。因此，异质性不应被视为Bug，而应被看作是一个关键的Feature。

- 应对异质性的GNN设计:

  为了让GNN能够在异质图上有效工作，研究界已经提出了一系列精巧的设计：

  - **架构改进：** 包括在GNN层中分离自我嵌入（ego-embedding）和邻居嵌入（neighbor-embedding）的变换 11，使得模型可以独立地学习如何处理自身信息和来自不同邻居的信息；引入高阶邻居信息，以接触到更远处可能存在的同质节点；以及将GNN不同层的输出结合起来，因为在异质图中，浅层和深层可能捕捉到不同尺度的信息。一篇发表在

    **NeurIPS 2024**的关于链接预测的论文 12 系统地研究了特征异质性的影响，并从理论和实验上证明了采用可学习的解码器以及在消息传递中分离自我与邻居嵌入对于处理异质性的重要性，这为我们设计无监督模型提供了宝贵的借鉴。

  - **信息论视角：** 一篇即将发表于**ICML 2025**的论文 33 从信息论的角度提供了一个深刻的见解。该研究发现，在同质图上，随着GNN层数的加深，节点从邻居处获得的新信息会逐渐减少（信息瓶颈）；但在异质图上，由于邻居节点本身就具有不同的语义，GNN的每一层都可能为中心节点带来全新的、有价值的信息。因此，联合考虑一个节点在所有GNN层的嵌入表示，对于进行不确定性估计（这与OOD检测密切相关）至关重要。

综合以上分析，我们的图构建策略不应回避甚至抑制异质性，而应主动地、有目的地去挖掘和建模异质性。我们可以设计专门的LLM Prompt，让它不再寻找“相似”关系，而是去识别句子对之间的“语义张力”（Semantic Tension），例如**矛盾（Contradiction）、转折（Turn）、意外（Surprise）、因果倒置（Causal Inversion）**等。这样构建出的图，我们称之为“语义张力图”。在这个图中，边的存在及其权重直接量化了节点间的异质程度。一个节点如果拥有大量高权重的异质边，那么它本身就具有更高的OOD嫌疑。通过这种方式，图结构本身就从一个被动的表示载体，转变成了OOD检测器的一个主动、有机的组成部分。



## 第三部分：最终算法方案详述



基于前两部分的分析与洞察，我们提出了一个专为无监督句子级文本OOD检测设计的全新框架，名为**HGC-GP (Heterophilic Graph Causal-Disentanglement with Generative Prototyping)**。本部分将详细阐述该框架的整体架构与每个核心模块的计算流程。



### 3.1 框架总览



HGC-GP框架是一个模块化的系统，旨在协同利用LLM的语义推理能力、GNN的结构学习能力、因果推断的表征解耦能力以及生成模型的分布外推能力。整个框架由以下四个核心模块构成，其协同工作流程如下图所示：

- 模块一：LLM驱动的异质关系图构建 (LLM-driven Heterophilic Graph Construction)

  该模块负责将输入的原始句子集合转化为一个加权的、无向的图。与传统基于相似性的建图方法不同，本模块利用LLM的推理能力，专注于挖掘和量化句子之间存在的“语义张力”（如对立、转折、补充等），从而构建一个能够凸显OOD样本结构异常性的异质图。

- 模块二：因果解耦的语义表征 (Causal-Disentangled Semantic Representation)

  该模块的目标是将每个句子的语义表示分解为两个相互独立的子空间：一个捕捉句子核心、稳定、与主题内容紧密相关的“因果特征”（Causal Features），另一个则捕捉句子表层、易变、与风格或表达方式相关的“混淆特征”（Spurious Features）。这种解耦有助于模型区分内容上的新颖（真正的OOD）和风格上的奇特（可能是ID的变体）。

- 模块三：生成式混合原型学习 (Generative Mixture-of-Prototypes Learning)

  该模块在解耦出的“因果特征”空间中进行操作。它首先借鉴PALM的思想，使用多个原型来为ID数据构建一个紧凑且多模态的表示。然后，它进一步借鉴GOLD的思想，引入一个对抗性生成器，该生成器被训练用于在ID原型簇的边界外生成高质量的“伪OOD”样本。这些生成的样本随后被用于校准和强化OOD检测器。

- 模块四：异质图OOD分数传播与校准 (Heterophilic Graph OOD Score Propagation and Calibration)

  该模块是最终决策的核心。它首先基于节点的因果偏离度、混淆强度和结构不稳定性计算一个初始的OOD分数。然后，利用一个专为异质性设计的GNN模型，在图上对这些分数进行传播和更新，使得节点的最终OOD分数能够综合其自身属性和其在“语义张力图”中的邻域环境。



### 3.2 核心模块与计算流程



下面，我们将对每个模块的计算步骤进行细粒度的、对齐的说明。



#### **Step 1: 异质感知图构建 (Heterophily-Aware Graph Construction)**



- **步骤名称 (Step Name):** 异质感知图构建

- **输入 (Input):** 原始句子集合 Din={s1,s2,...,sN}。

- **输出 (Output):** 一个加权的无向图 G=(V,E,W)，其中节点集 V 对应句子集合，边集 E 表示句子间存在显著的异质关系，权重矩阵 W∈RN×N 量化了这种关系的强度。

- **核心计算过程 (Computation Process):**

  1. **候选边生成 (Candidate Edge Generation):** 直接对所有 N2 对句子进行LLM推理的成本过高。为了提高效率，我们首先进行一步筛选。使用一个轻量级但高效的语义相似度模型（例如，基于Sentence-BERT的SimCSE）计算每对句子 (si,sj) 的余弦相似度。对于每个句子 si，我们只保留与其语义相似度排名前 K 的句子作为其候选邻居。这一步的假设是，有意义的异质关系（如驳斥、转折）通常也发生在主题相关的句子之间。

  2. **LLM关系推理与加权 (LLM-based Relation Inference and Weighting):** 对上一步筛选出的每一对候选句子 (si,sj)，我们设计一个结构化的Prompt，并调用LLM（如GPT-4o或一个经过微调的开源模型）进行推理。该Prompt的设计至关重要，它需要引导LLM超越简单的相似性判断，去理解两者之间深层的逻辑和语义关系 34。一个示例Prompt如下：

     ```
     You are an expert in analyzing textual relationships. Given two sentences, Sentence A and Sentence B, your task is to identify the primary relationship between them from the following categories:
      Contradiction: Sentence B directly contradicts or refutes the main point of Sentence A.
      Elaboration with a Twist: Sentence B elaborates on Sentence A but introduces a new, unexpected, or contrasting element.
      Unexpected Turn: Sentence B presents a consequence or a follow-up to Sentence A that is surprising or unconventional.
      Premise-Anomaly: Sentence A provides a premise, and Sentence B provides a conclusion that is logically possible but highly unusual or anomalous given the premise.
      Neutral/Similar: No significant tension or heterophilic relationship exists.
     
     Analyze the following pair:
     Sentence A: "{s_i}"
     Sentence B: "{s_j}"
     
     Output your answer in JSON format with two keys: "relationship_type" (an integer from 1 to 5) and "confidence_score" (a float from 0.0 to 1.0, indicating your certainty).
     ```

  3. 图构建与权重计算 (Graph Construction and Weight Computation):

     如果LLM返回的关系类型为1到4（即非中性关系），我们就在节点 i 和节点 j 之间建立一条边。该边的权重 wij 直接由LLM输出的置信度分数决定。如果关系类型为5，则不建立边（或权重为0）。

     wij=wji={LLM-Conf(si,sj)0if LLM-Type(si,sj)∈{1,2,3,4}otherwise

     通过这个过程，我们得到一个稀疏的、加权的“语义张力图”，其边和权重直接反映了文本网络中的异质性结构。



#### **Step 2: 基于信息瓶颈的因果语义解耦 (Causal Semantic Disentanglement via IB)**



- **步骤名称 (Step Name):** 基于信息瓶颈的因果语义解耦

- **输入 (Input):** 数据集中的任意一个句子 si。

- **输出 (Output):** 该句子的因果嵌入 zci∈Rdc 和混淆嵌入 zsi∈Rds。

- 核心计算过程 (Computation Process):

  此模块借鉴了SLOGAN (ICML 2025) 36 中用于图域自适应的因果解耦思想，并将其应用于我们的句子级OOD检测任务。我们使用一个预训练的语言模型（如BERT或DistilBERT以提高效率）作为基础编码器 

  Eϕ。

  1. **基础嵌入 (Base Embedding):** 首先，将句子 si 通过编码器 Eϕ 得到其高维上下文表示 hi=Eϕ(si)。

  2. 解耦投影 (Disentanglement Projection): 我们假设 hi 中混合了因果信息和混淆信息。为了分离它们，我们设计了两个独立的投影头（可以是简单的线性层或浅层MLP），Pc 和 Ps，将 hi 分别映射到因果空间和混淆空间：

     

     zci=Pc(hi)

     zsi=Ps(hi)

  3. 信息瓶颈约束训练 (Training with Information Bottleneck Constraint):

     为了让 zci 和 zsi 真正学习到我们期望的信息，我们需要一个自监督的训练目标。我们遵循信息瓶颈（Information Bottleneck, IB）原理。其核心思想是，我们希望因果嵌入 zci 对于某个（假想的）下游任务 Y 来说是“充分”的，而混淆嵌入 zsi 对于 Y 来说是“无关”的。在无监督设定下，我们可以构建一个自监督的 pretext task 来产生伪标签 Y。例如，我们可以通过对句子进行数据增强（如回译、同义词替换）来构建正样本对，任务 Y 就是判断两个增强后的句子是否源于同一个原始句子。

     IB的优化目标是最大化 zc 和 Y 之间的互信息 I(Zc;Y)，同时最小化 zc 和原始输入 X 之间的互信息 I(X;Zc)，以强迫 zc 只保留与 Y 相关的“压缩”信息。同时，我们希望 zs 保留所有与 Y 无关的信息。一个可行的损失函数可以设计为：

     

     LIB=−I(Zc;Y)+βI(X;Zc)+γI(Zs;Y)

     

     其中 β 和 γ 是超参数。在实践中，这些互信息项通常通过其变分下界（Variational Lower Bounds）来近似和优化。通过优化这个损失函数，模型被激励将与自监督任务（代表核心语义）相关的信息编码到 zc 中，而将其他信息（如句法结构、用词习惯等风格伪影）编码到 zs 中。



#### **Step 3: 生成式混合原型学习 (Generative Mixture-of-Prototypes Learning)**



- **步骤名称 (Step Name):** 生成式混合原型学习

- **输入 (Input):** 所有ID样本的因果嵌入集合 {zc1,zc2,...,zcN}。

- **输出 (Output):** 一组ID数据在因果空间中的原型 {p1,p2,...,pK}，以及一个训练好的、能够生成“边界”伪OOD样本的生成器 Gθ。

- 核心计算过程 (Computation Process):

  该模块是本框架的核心创新之一，它将PALM 16 的多原型建模能力与

  **GOLD** 6 的对抗生成能力进行了深度融合。

  1. 混合原型学习 (Mixture-of-Prototypes Learning):

     我们首先在ID因果嵌入集合 {zci}i=1N 上运行一个类似PALM的聚类过程。具体来说，我们可以使用期望最大化（EM）算法或者一种软分配机制来迭代地更新 K 个原型 {pk}k=1K 的位置和样本到原型的分配概率。其目标是最小化所有ID样本到其关联原型的加权距离总和，从而在因果空间中形成 K 个紧凑的ID语义簇。

  2. 对抗性边界生成 (Adversarial Boundary Generation):

     在获得ID数据的原型表示后，我们引入一个对抗训练过程来生成高质量的伪OOD样本。该过程包含一个生成器 Gθ 和一个判别器 Dψ（它本身就是OOD检测器的一部分）。

     - **生成器 (Generator) \**Gθ\**:** 输入为一个从标准正态分布中采样的随机噪声向量 z∼N(0,I)，输出为一个伪OOD因果嵌入 z~c=Gθ(z)。

     - 判别器 (Discriminator) Dψ: 我们将其设计为一个能量模型。它接收一个因果嵌入（真实的 zci 或生成的 z~c）作为输入，输出一个标量能量值。训练目标是使真实ID样本的能量尽可能低，而伪OOD样本的能量尽可能高。

       $$ \mathcal{L}D = \frac{1}{N}\sum{i=1}^N \text{Energy}\psi(z_c^i) - \mathbb{E}{z \sim \mathcal{N}(0, I)}[\text{Energy}\psi(G\theta(z))] $$

     - 生成器的创新损失函数: 生成器的目标不仅仅是生成高能量的样本来“欺骗”判别器。我们对其施加一个额外的约束，引导它在ID原型簇定义的流形边界外生成样本。这通过一个结合了能量项和原型距离项的复合损失函数来实现：

       $$ \mathcal{L}G = \mathbb{E}{z \sim \mathcal{N}(0, I)}[\text{Energy}\psi(G\theta(z))] + \lambda \cdot \mathbb{E}{z \sim \mathcal{N}(0, I)} \left[ \max\left(0, m - \min{k} |G_\theta(z) - p_k|^2\right) \right] $$

       第一项是标准的对抗损失，驱动生成器产生高能量样本。第二项是原型边界铰链损失（Prototype-Boundary Hinge Loss），其中 m 是一个预设的边界（margin）。该项惩罚那些生成得“太靠近”任何一个ID原型的样本（即与最近原型的距离小于 m 的样本），从而迫使生成器在ID分布的边缘地带进行探索和生成。

       通过交替优化 LD 和 LG，我们不仅能学到一个能够区分ID和OOD的能量函数 Dψ，还能得到一个专门生成“边界困难样本”的生成器 Gθ。这些生成的样本可以被加入到训练中，极大地增强了检测器的鲁棒性。



#### **Step 4: 异质图OOD分数传播与最终决策 (Heterophilic GNN Score Propagation & Final Decision)**



- **步骤名称 (Step Name):** 异质图OOD分数传播与最终决策

- **输入 (Input):** 异质图 G=(V,E,W)，所有节点的因果嵌入 {zci} 和混淆嵌入 {zsi}，以及学习到的ID原型集 {pk}。

- **输出 (Output):** 每个节点 i 的最终OOD分数 Oi。

- **核心计算过程 (Computation Process):**

  1. 初始节点分数计算 (Initial Node Score Calculation):

     对于每个节点 i，我们从三个维度计算其初始的OOD嫌疑度：

     - 因果偏离度 (Causal Deviation Score) dci: 衡量节点的核心语义内容与ID数据主流语义的偏离程度。它被定义为节点因果嵌入 zci 到最近的ID原型的平方欧氏距离。

       

       dci=k∈{1,...,K}min∥zci−pk∥22

     - 混淆强度 (Spuriousness Magnitude Score) dsi: 衡量节点的表层风格或表达方式的异常程度。它被定义为节点混淆嵌入 zsi 的L2范数的平方。一个具有异常句法或用词的句子，其混淆嵌入的模长会较大。

       

       dsi=∥zsi∥22

     - 结构不稳定性 (Structural Instability Score) dgi: 衡量节点在其局部图环境中的“紧张”或“不和谐”程度。它被定义为节点在异质图 G 中的加权度数。一个节点的加权度数越高，意味着它与邻居之间存在越多的、越强的“语义张力”。

       

       dgi=j∈N(i)∑wij

  2. 异质图分数传播 (Heterophilic Graph Score Propagation):

     我们不直接在原始特征上进行消息传递，而是借鉴Score Propagation (NeurIPS 2024) 37 的思想，在这些更具信息量的OOD分数上进行传播。我们使用一个专为异质图设计的GNN层，例如一个带有自我-邻居分离机制的图卷积网络（类似Geom-GCN 10 或 H2GCN 29 的思想）。传播过程可以表示为：

     

     $$ \mathbf{d'}_i = \sigma \left( \mathbf{W}0 \cdot \mathbf{d}i + \sum{j \in \mathcal{N}(i)} \frac{w{ij}}{\sqrt{\hat{d}_i \hat{d}_j}} \mathbf{W}1 \cdot \mathbf{d}j \right) $$

     其中 di=[dci,dsi,dgi] 是节点 i 的初始分数向量，W0 和 W1 是可学习的权重矩阵，分别用于变换中心节点和邻居节点的分数。d^i 是节点 i 的加权度数，σ 是非线性激活函数。通过一层或多层这样的传播，每个节点的最终分数向量 $\mathbf{d'}i = [d'{c,i}, d'{s,i}, d'{g,i}]$ 将会融合其邻域的OOD信号。

  3. 最终OOD分数决策 (Final OOD Score Decision):

     节点的最终OOD分数 Oi 是其传播后各项分数的加权组合：

     

     Oi=α⋅dc,i′+β⋅ds,i′+γ⋅dg,i′

     

     其中，权重 α,β,γ 是超参数，可以在一个小的验证集上调整，或者通过一个无监督的目标（如最大化ID和生成伪OOD样本分数之间的间隔）来学习。这个最终分数综合了内容、风格和结构三个维度的异常信号，从而做出更鲁棒的OOD判断。



### 3.3 算法复杂度与端到端讨论



- 复杂度分析:

  HGC-GP框架的主要计算开销分布在几个部分：

  1. **图构建:** LLM调用的成本是主要部分。通过候选边筛选，我们将调用次数从 O(N2) 降低到 O(N⋅K)，其中 K≪N。这使得算法对于中等规模的数据集是可行的。
  2. **因果解耦与原型学习:** 这部分的训练复杂度主要取决于所用编码器的大小和训练的轮数，与标准深度学习模型训练相当。
  3. **GNN传播:** GNN的计算复杂度通常为 O(∣E∣⋅d)，其中 ∣E∣ 是边的数量， d 是特征维度。由于我们构建的是一个稀疏图，这部分计算非常高效。

- 关于端到端（End-to-End）训练的讨论:

  当前设计的HGC-GP是一个分阶段的框架，这具有明显的优势：模块化、可解释性强、易于调试和实现。每个模块都可以独立开发、测试和优化。例如，我们可以先构建好图，然后专注于训练表征学习和OOD检测模块。对于一个博士研究项目而言，一个逻辑清晰、每个部分都能被充分验证的模块化框架，比一个难以训练、难以分析的端到端“黑箱”模型更具学术价值和现实意义。

  尽管如此，理论上存在将整个流程端到端化的可能性。例如，图构建过程中的LLM推理步骤可以通过梯度直通估计器（Gumbel-Softmax trick for discrete choices）或策略梯度等强化学习方法变得可微，从而纳入到整个反向传播链条中。然而，这将极大地增加模型的训练复杂度和不稳定性，需要大量的工程技巧和计算资源。因此，我们建议在当前阶段坚持分阶段的设计，将端到端的探索作为一项富有挑战性的未来工作。



## 第四部分：实验设计与预期结果



为了系统地验证HGC-GP框架的有效性，我们设计了一套全面的实验方案，包括数据集选择、基线模型对比、评估指标、以及深入的消融研究。



### 4.1 数据集与预处理



- 目标实验数据集分析:

  (此部分需要你提供具体的数据集描述。以下为一个示例性分析框架，请根据你的实际数据集进行填充。)

  假设我们的目标数据集是CLINC150的一个变体，它包含了150个意图类别，分布在10个领域。我们将对其进行如下分析：

  - **文本长度分布:** 句子的平均长度、长度方差等。这影响我们选择LLM编码器和处理上下文的方式。如果句子普遍较短，复杂的长文本依赖模型可能不是必需的。
  - **主题多样性:** 150个类别提供了丰富的主题。我们将分析类别间的语义相似度，以识别哪些类别在语义上更接近，哪些更疏远。这对于OOD划分至关重要。
  - **句子间潜在关系:** 我们将随机抽样句子对，初步评估其中是否存在我们定义的异质关系（如对立、转折），以验证我们提出的“语义张力图”构建策略在该数据集上的适用性。

- 数据划分 (Data Partitioning):

  我们将采用标准的OOD检测实验设置。例如，从CLINC150的10个领域中，随机选择6个领域（例如，banking, credit_cards, kitchen_and_dining, home, utility, auto_and_commute）下的所有意图类别作为ID数据。剩下的4个领域（例如，travel, meta, work, small_talk）下的所有意图类别则作为OOD数据。这种划分方式确保了ID和OOD数据在领域（Domain）级别上是分离的，是一种具有挑战性且符合现实场景的设定 。在训练阶段，模型只能看到ID数据；在测试阶段，模型需要区分来自ID和OOD领域的句子。



### 4.2 基线模型 (Baselines)



我们将选择一系列具有代表性的、覆盖不同技术路线的基线模型进行公平比较：

- **传统/统计方法:**
  - **Mahalanobis:** 一种经典的基于距离的方法，在深度特征空间中计算样本与ID数据高斯分布中心之间的马氏距离。
- **基于深度学习的通用OOD检测方法:**
  - **Deep SVDD:** 学习一个最小的超球面来包围ID数据的特征表示。
  - **Vanilla Autoencoder:** 基于重构误差的基线模型 14。
  - **MSP (Max Softmax Probability):** 一个简单但强大的基线，使用在ID类别上训练的分类器的最大softmax输出概率作为ID置信度。
  - **Energy-based:** 使用在ID数据上训练的模型的能量值作为OOD分数 17。
- **先进的无监督OOD检测方法:**
  - **PALM (ICLR 2024):** 代表了最先进的基于多原型学习的方法，是我们重要的比较对象 16。
  - **GOLD (ICLR 2025):** 代表了最先进的基于生成的方法，其不依赖外部数据的特性使其成为一个极具竞争力的基线 6。
- **图OOD检测方法:**
  - **GNNSafe:** 一种将能量模型与GNN结合的早期代表性工作。
  - **DeGEM (ICML 2024):** 当前最先进的图OOD检测方法之一，尤其擅长处理异质性 13。为了公平比较，我们将使用其无监督版本，并采用与我们方法相同的图构建策略（或其默认策略）进行测试。
- **LLM/GNN融合方法:**
  - **GNN-centric Baseline:** 我们将构建一个简单的融合基线，即使用一个强大的LLM（如BERT-large）提取句子嵌入，然后输入到一个标准的GCN或GAT中进行节点分类（在ID数据上训练），并使用MSP或能量作为OOD分数。



### 4.3 评估指标



我们将采用OOD检测领域的标准评估指标来全面衡量模型性能：

- **AUROC (Area Under the Receiver Operating Characteristic Curve):** 衡量模型在所有可能阈值下区分ID和OOD样本的总体能力。AUROC值越接近1，性能越好。
- **AUPR (Area Under the Precision-Recall Curve):** 当ID和OOD样本数量不平衡时，AUPR能更敏感地反映模型性能。分为AUPR-In和AUPR-Out。
- **FPR@95TPR (False Positive Rate at 95% True Positive Rate):** 衡量当模型能正确识别95%的ID样本时，将多少OOD样本错误地判断为ID。该指标在安全攸关的应用中尤为重要，值越低越好。



### 4.4 消融实验 (Ablation Study)



消融实验是验证我们框架设计合理性的核心。我们将通过移除或替换HGC-GP中的关键模块，来检验每个模块的独立贡献。这将是一篇顶级会议论文审稿人高度关注的部分。

| 模型变体 (Model Variant)          | 描述                                                         | 预期结果分析                                                 |
| --------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **HGC-GP (Full Model)**           | 完整的最终方案。                                             | 作为性能基准 (upper bound)。                                 |
| **1. w/o Heterophily-Graph**      | 使用简单的kNN图（基于语义相似度）替换LLM驱动的异质图。       | 性能会显著下降，证明主动挖掘和利用“语义张力”对于OOD检测至关重要。 |
| **2. w/o Causal-Disentanglement** | 不进行因果解耦，直接在原始LLM嵌入上进行原型学习和OOD检测。   | 性能下降，尤其是在处理风格/句法OOD时。这表明解耦出稳定的因果特征有助于抵抗伪相关性的干扰。 |
| **3. w/o Mixture-of-Prototypes**  | 在原型学习步骤中，设置原型数量 K=1，即退化为单原型方法。     | 性能下降，说明ID数据在因果空间中仍是多模态的，多原型建模是必要的。 |
| **4. w/o Generative-Module**      | 移除对抗生成器，仅使用ID样本到原型的距离作为OOD分数。        | 性能下降，证明通过生成“边界”伪OOD样本来校准检测器能有效提升模型的判别边界。 |
| **5. w/o GNN-Propagation**        | 不使用GNN传播分数，直接使用初始计算的节点分数加权求和作为最终决策。 | 性能下降，证明利用图结构信息，让节点OOD分数受其邻域环境影响，能够修正孤立判断的错误。 |



### 4.5 预期结果



1. **整体性能:** 我们预期**HGC-GP (Full Model)** 将在所有主要评估指标（AUROC, AUPR, FPR@95TPR）上全面且显著地超越所有基线模型。尤其相较于GOLD和DeGEM，我们的方法通过融合因果解耦和更具信息量的异质图，有望在处理复杂的文本OOD场景时展现出更强的鲁棒性。
2. **消融实验结果:** 预期上述表格中的每个变体都会导致性能的明显下降，从而有力地证明我们提出的**异质图构建、因果解耦、生成式多原型学习、GNN分数传播**这四个核心模块都是不可或缺且相辅相成的。
3. **案例分析与可解释性:** 我们将挑选一些典型的ID和OOD句子进行案例分析。预期结果会显示，对于一个内容新颖的OOD句子，其**因果偏离度**会很高；对于一个风格怪异但内容常规的句子，其**混淆强度**会很高；对于一个与周围句子形成强烈对立的OOD句子，其**结构不稳定性**会很高。这将直观地展示我们模型的可解释性优势。



## 第五部分：结论与未来展望





### 5.1 结论



本研究旨在解决无监督句子级文本OOD检测这一充满挑战的前沿问题。面对现有方法在处理文本复杂性、融合多模态信息以及应对无监督设定等方面的局限，我们提出了一个名为**HGC-GP**的全新框架。该框架通过引入四重核心创新，系统性地应对了这些挑战：

1. **LLM驱动的异质图构建**，将异质性从“挑战”转化为“信号”，使图结构本身成为OOD检测的有力工具；
2. **基于信息瓶颈的因果语义解耦**，使模型能够区分内容新颖性与风格异常，提升了检测的鲁棒性和可解释性；
3. **生成式混合原型学习**，巧妙地结合了多原型建模和对抗生成，在不依赖外部数据的情况下，学习到紧凑的ID分布并有效探索其边界；
4. **异质图OOD分数传播**，综合了节点的内在属性和外在结构环境，做出了更全面的判断。

综上所述，HGC-GP框架为无监督文本OOD检测提供了一个功能强大、逻辑自洽、可解释性强且具备前沿性的解决方案。我们预期的实验结果将证明其相较于现有最先进方法的优越性，并为未来在开放世界中部署安全、可靠的NLP系统提供一种新的可能。



### 5.2 未来展望



尽管HGC-GP框架展现了巨大的潜力，但仍有若干方向值得在未来的工作中进一步探索：

- **端到端训练范式的探索:** 如前文所述，当前框架是模块化的。探索如何通过可微分的图构建技术（如使用策略梯度或Gumbel-Softmax）将整个流程整合为一个端到端的训练框架，是一个富有挑战但可能带来性能提升的方向。
- **向多模态OOD检测扩展:** 现实世界的信息往往是多模态的（如文本配图像、语音配字幕）。如何将HGC-GP框架中的思想扩展到多模态OOD检测任务，例如，构建一个跨模态的异质图，并解耦出模态共享的因果特征和模态特有的混淆特征，将是一个极具价值的研究课题。
- **效率与可扩展性优化:** 当前框架中的LLM调用是主要的效率瓶颈。研究更高效的LLM推理技术（如模型蒸馏、量化）和更智能的图构建采样策略，以将本框架应用于更大规模（百万甚至千万级节点）的数据集，是其走向实际应用的关键。
- **OOD检测与OOD泛化的统一:** OOD检测（Detection）关注于“识别”未知，而OOD泛化（Generalization）关注于在未知分布上“表现良好” 38。两者紧密相关但目标不同。未来的研究可以探索如何在一个统一的框架下同时优化这两个目标，例如，利用OOD检测模块识别出高不确定性样本，并指导模型在这些样本上进行自适应的泛化学习。