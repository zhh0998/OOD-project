#!/usr/bin/env python3
"""
ä¸‰æ–¹æ¡ˆç»Ÿä¸€OODæ£€æµ‹å¯¹æ¯”å®éªŒ
=====================================
å…¬å¹³å¯¹æ¯”HMCENã€C4-TDAã€CP-ABR++åœ¨OODæ£€æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import Data
from sklearn.metrics import roc_auc_score, average_precision_score
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# ==================== æ•°æ®åŠ è½½ ====================

def load_clinc150_for_ood(n_samples=2000, ood_ratio=0.3):
    """
    åŠ è½½CLINC150æ•°æ®ç”¨äºOODæ£€æµ‹

    è¿”å›:
        data: PyG Dataå¯¹è±¡ï¼ŒåŒ…å«:
            - x: èŠ‚ç‚¹ç‰¹å¾
            - edge_index: è¾¹
            - y: èŠ‚ç‚¹æ ‡ç­¾ï¼ˆIDç±»ï¼‰
            - ood_labels: OODæ ‡ç­¾ï¼ˆ0=ID, 1=OODï¼‰
            - id_mask: IDæ ·æœ¬mask
            - ood_mask: OODæ ·æœ¬mask
    """
    from datasets import load_dataset
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity

    print("åŠ è½½CLINC150æ•°æ®é›†...")
    dataset = load_dataset('clinc_oos', 'plus')

    # é‡‡æ ·
    test_data = dataset['test']
    indices = np.random.choice(len(test_data), n_samples, replace=False)
    texts = [test_data[i]['text'] for i in indices]
    labels = [test_data[i]['intent'] for i in indices]

    # TF-IDFç‰¹å¾
    vectorizer = TfidfVectorizer(max_features=300)
    features = vectorizer.fit_transform(texts).toarray()

    # æ„å»ºå›¾ï¼ˆk-NNï¼Œk=20ï¼‰
    k = 20
    knn = NearestNeighbors(n_neighbors=k+1, metric='cosine')
    knn.fit(features)
    distances, indices_knn = knn.kneighbors(features)

    edge_list = []
    for i in range(n_samples):
        for j in range(1, k+1):  # è·³è¿‡è‡ªå·±
            neighbor = indices_knn[i, j]
            edge_list.append([i, neighbor])
            edge_list.append([neighbor, i])  # æ— å‘å›¾

    edge_index = torch.tensor(edge_list, dtype=torch.long).t()

    # ç¡®å®šIDå’ŒOOD
    unique_labels = list(set(labels))
    n_ood_classes = int(len(unique_labels) * ood_ratio)
    ood_classes = np.random.choice(unique_labels, n_ood_classes, replace=False)

    ood_labels = np.array([1 if label in ood_classes else 0 for label in labels])
    id_mask = ood_labels == 0
    ood_mask = ood_labels == 1

    # è½¬æ¢æ ‡ç­¾ï¼ˆåªå¯¹IDç±»ï¼‰
    label_map = {label: i for i, label in enumerate(unique_labels) if label not in ood_classes}
    y = np.array([label_map.get(label, -1) for label in labels])

    data = Data(
        x=torch.tensor(features, dtype=torch.float32),
        edge_index=edge_index,
        y=torch.tensor(y, dtype=torch.long),
        ood_labels=torch.tensor(ood_labels, dtype=torch.long),
        id_mask=torch.tensor(id_mask, dtype=torch.bool),
        ood_mask=torch.tensor(ood_mask, dtype=torch.bool),
        num_classes=len(label_map)
    )

    print(f"æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬: {n_samples}")
    print(f"  IDæ ·æœ¬: {id_mask.sum()} ({id_mask.sum()/n_samples*100:.1f}%)")
    print(f"  OODæ ·æœ¬: {ood_mask.sum()} ({ood_mask.sum()/n_samples*100:.1f}%)")
    print(f"  IDç±»åˆ«æ•°: {len(label_map)}")
    print(f"  OODç±»åˆ«æ•°: {n_ood_classes}")

    return data

# ==================== æ–¹æ¡ˆ1: HMCEN-C ====================

class HMCEN_C(nn.Module):
    """
    HMCEN with linear fusion (æ–¹æ¡ˆC)
    """
    def __init__(self, input_dim, hidden_dim=128):
        super().__init__()
        # åŒé…æ€§åˆ†æ”¯
        self.homo_branch = GCNConv(input_dim, hidden_dim)
        # å¼‚é…æ€§åˆ†æ”¯
        self.hetero_branch = GCNConv(input_dim, hidden_dim)

        # èåˆä¸åˆ†ç±»
        self.fusion = nn.Linear(hidden_dim, 64)
        self.classifier = nn.Linear(64, 2)  # ID/OODäºŒåˆ†ç±»

    def forward(self, x, edge_index, h_node):
        """
        h_node: èŠ‚ç‚¹å¼‚é…æ€§ [0,1]
        """
        # ä¸¤ä¸ªåˆ†æ”¯
        h_homo = self.homo_branch(x, edge_index)
        h_hetero = self.hetero_branch(x, edge_index)

        # çº¿æ€§é—¨æ§ (æ–¹æ¡ˆC)
        alpha = (1.0 - h_node).unsqueeze(-1)

        # èåˆ
        h_fused = alpha * h_homo + (1 - alpha) * h_hetero
        h_fused = F.relu(h_fused)

        h_final = self.fusion(h_fused)
        h_final = F.relu(h_final)

        logits = self.classifier(h_final)
        return logits

    def predict_ood_score(self, x, edge_index, h_node):
        """è¿”å›OODåˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¯èƒ½æ˜¯OODï¼‰"""
        logits = self.forward(x, edge_index, h_node)
        probs = F.softmax(logits, dim=-1)
        return probs[:, 1]  # OODç±»çš„æ¦‚ç‡

def train_hmcen(data, epochs=200, lr=0.01, seed=42):
    """è®­ç»ƒHMCEN-C"""
    torch.manual_seed(seed)
    np.random.seed(seed)

    # è®¡ç®—å¼‚é…æ€§
    h_node = compute_heterophily_pseudo(data)

    model = HMCEN_C(input_dim=data.x.shape[1])
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    # è®­ç»ƒï¼ˆåœ¨IDæ•°æ®ä¸Šï¼‰
    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()

        logits = model(data.x, data.edge_index, h_node)

        # åœ¨IDæ•°æ®ä¸Šè®­ç»ƒï¼ˆç”¨å¼‚é…æ€§ä½œä¸ºä¼ªæ ‡ç­¾ï¼‰
        # é«˜å¼‚é…æ€§ â†’ å¯èƒ½OOD (æ ‡ç­¾1)
        train_mask = data.id_mask
        train_labels = (h_node[train_mask] > h_node[train_mask].median()).long()

        loss = F.cross_entropy(logits[train_mask], train_labels)

        loss.backward()
        optimizer.step()

        if (epoch+1) % 50 == 0:
            print(f"  Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")

    # é¢„æµ‹OODåˆ†æ•°
    model.eval()
    with torch.no_grad():
        ood_scores = model.predict_ood_score(data.x, data.edge_index, h_node)

    return ood_scores.cpu().numpy()

# ==================== æ–¹æ¡ˆ2: C4-TDA-OOD ====================

def compute_betti_numbers_simple(edge_index, num_nodes):
    """
    è®¡ç®—Bettiæ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
    ä½¿ç”¨scipyè€Œéripser
    """
    from scipy.sparse import csr_matrix
    from scipy.sparse.csgraph import connected_components

    # Î²â‚€: è¿é€šåˆ†é‡æ•° - 1
    adj = csr_matrix(
        (np.ones(edge_index.shape[1]),
         (edge_index[0].numpy(), edge_index[1].numpy())),
        shape=(num_nodes, num_nodes)
    )
    n_components, labels = connected_components(adj, directed=False)
    beta_0 = n_components - 1

    # Î²â‚: ç¯çš„æ•°é‡ï¼ˆç®€åŒ–ä¼°è®¡ï¼‰
    # Î²â‚ â‰ˆ |E| - |V| + |C|ï¼ˆæ¬§æ‹‰ç‰¹å¾ï¼‰
    n_edges = edge_index.shape[1] // 2  # æ— å‘å›¾
    n_vertices = num_nodes
    beta_1 = max(0, n_edges - n_vertices + n_components)

    return beta_0, beta_1

def c4tda_ood_detection(data, use_calibration=True):
    """
    C4-TDAç”¨äºOODæ£€æµ‹

    ä¸‰ç§æ–¹æ³•:
    1. Î²â‚ç›´æ¥ä½œä¸ºOODåˆ†æ•°
    2. h*Î²â‚ï¼ˆåˆ©ç”¨å¼‚é…æ€§ï¼‰
    3. è½»é‡æ ¡å‡†ï¼ˆé€»è¾‘å›å½’ï¼‰
    """
    # è®¡ç®—å¼‚é…æ€§
    h_node = compute_heterophily_pseudo(data)

    # è®¡ç®—èŠ‚ç‚¹çº§Bettiæ•°ï¼ˆä½¿ç”¨ego-graphï¼‰
    num_nodes = data.x.shape[0]
    betti_0_list = []
    betti_1_list = []

    print("  è®¡ç®—æ‹“æ‰‘ç‰¹å¾...")
    for node_id in range(num_nodes):
        # æå–1-hop ego-graph
        neighbors = data.edge_index[1][data.edge_index[0] == node_id]
        subgraph_nodes = torch.cat([torch.tensor([node_id]), neighbors])

        # å­å›¾è¾¹
        mask = torch.isin(data.edge_index[0], subgraph_nodes) & \
               torch.isin(data.edge_index[1], subgraph_nodes)
        subgraph_edges = data.edge_index[:, mask]

        # é‡æ–°ç¼–å·
        node_map = {n.item(): i for i, n in enumerate(subgraph_nodes)}
        subgraph_edges_reindexed = torch.tensor([
            [node_map[e[0].item()] for e in subgraph_edges.t()],
            [node_map[e[1].item()] for e in subgraph_edges.t()]
        ])

        # è®¡ç®—Bettiæ•°
        beta_0, beta_1 = compute_betti_numbers_simple(
            subgraph_edges_reindexed,
            len(subgraph_nodes)
        )
        betti_0_list.append(beta_0)
        betti_1_list.append(beta_1)

    betti_0 = np.array(betti_0_list)
    betti_1 = np.array(betti_1_list)

    # æ–¹æ³•1: Î²â‚ç›´æ¥
    ood_scores_beta1 = betti_1

    # æ–¹æ³•2: h*Î²â‚
    ood_scores_h_beta1 = h_node.numpy() * betti_1

    # æ–¹æ³•3: è½»é‡æ ¡å‡†
    if use_calibration:
        from sklearn.linear_model import LogisticRegression
        from torch_geometric.utils import degree

        # ç‰¹å¾: [h, Î²â‚€, Î²â‚, degree]
        degrees = degree(data.edge_index[0], num_nodes=num_nodes).numpy()
        features = np.column_stack([
            h_node.numpy(),
            betti_0,
            betti_1,
            degrees
        ])

        # åœ¨IDæ•°æ®ä¸Šè®­ç»ƒ
        train_mask = data.id_mask.numpy()
        train_labels = (h_node[data.id_mask] > h_node[data.id_mask].median()).numpy().astype(int)

        clf = LogisticRegression(max_iter=1000, random_state=42)
        clf.fit(features[train_mask], train_labels)

        ood_scores_calibrated = clf.predict_proba(features)[:, 1]
    else:
        ood_scores_calibrated = ood_scores_h_beta1

    return {
        'beta1': ood_scores_beta1,
        'h_beta1': ood_scores_h_beta1,
        'calibrated': ood_scores_calibrated
    }

# ==================== æ–¹æ¡ˆ3: CP-ABR++ ====================

class CP_ABR_Plus(nn.Module):
    """
    ç®€åŒ–ç‰ˆCP-ABR++ï¼ˆçº§è”å¼‚é…æ€§æ„ŸçŸ¥ï¼‰
    """
    def __init__(self, input_dim, hidden_dim=128):
        super().__init__()
        # Stage 1: Coarse detector
        self.coarse_gnn = GCNConv(input_dim, hidden_dim)
        self.coarse_classifier = nn.Linear(hidden_dim, 2)

        # Stage 2: Fine detector (å¼‚é…æ€§æ„ŸçŸ¥)
        self.fine_gnn = GCNConv(hidden_dim, hidden_dim)
        self.fine_classifier = nn.Linear(hidden_dim, 2)

    def forward(self, x, edge_index, h_node):
        # Stage 1
        h1 = F.relu(self.coarse_gnn(x, edge_index))
        logits_coarse = self.coarse_classifier(h1)

        # Stage 2 (å¼‚é…æ€§è‡ªé€‚åº”)
        alpha = h_node.unsqueeze(-1)
        h2 = F.relu(self.fine_gnn(h1, edge_index))
        h2_adaptive = alpha * h2 + (1-alpha) * h1
        logits_fine = self.fine_classifier(h2_adaptive)

        # èåˆ
        logits = 0.5 * logits_coarse + 0.5 * logits_fine
        return logits

def train_cp_abr(data, epochs=200, lr=0.01, seed=42):
    """è®­ç»ƒCP-ABR++"""
    torch.manual_seed(seed)
    np.random.seed(seed)

    h_node = compute_heterophily_pseudo(data)

    model = CP_ABR_Plus(input_dim=data.x.shape[1])
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    model.train()
    for epoch in range(epochs):
        optimizer.zero_grad()

        logits = model(data.x, data.edge_index, h_node)

        train_mask = data.id_mask
        train_labels = (h_node[train_mask] > h_node[train_mask].median()).long()

        loss = F.cross_entropy(logits[train_mask], train_labels)

        loss.backward()
        optimizer.step()

        if (epoch+1) % 50 == 0:
            print(f"  Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")

    model.eval()
    with torch.no_grad():
        logits = model(data.x, data.edge_index, h_node)
        ood_scores = F.softmax(logits, dim=-1)[:, 1]

    return ood_scores.cpu().numpy()

# ==================== è¾…åŠ©å‡½æ•° ====================

def compute_heterophily_pseudo(data):
    """
    è®¡ç®—ä¼ªå¼‚é…æ€§ï¼ˆæµ‹è¯•æ—¶æ— æ ‡ç­¾å¯ç”¨ï¼‰
    ä½¿ç”¨ç‰¹å¾ç›¸ä¼¼åº¦ä¼°è®¡
    """
    from torch_geometric.utils import degree

    num_nodes = data.x.shape[0]
    h_node = torch.zeros(num_nodes)

    for v in range(num_nodes):
        neighbors = data.edge_index[1][data.edge_index[0] == v]
        if len(neighbors) > 0:
            # ç‰¹å¾ä¸ç›¸ä¼¼åº¦
            feat_sim = F.cosine_similarity(
                data.x[v].unsqueeze(0),
                data.x[neighbors],
                dim=1
            )
            h_node[v] = 1 - feat_sim.mean()  # ä¸ç›¸ä¼¼åº¦ä½œä¸ºä¼ªå¼‚é…æ€§

    return h_node

def compute_fpr95(y_true, y_scores):
    """è®¡ç®—FPR@95% TPR"""
    from sklearn.metrics import roc_curve
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    idx = np.argmin(np.abs(tpr - 0.95))
    return fpr[idx]

# ==================== ä¸»å®éªŒ ====================

def run_experiment(seeds=[42, 2024, 2025]):
    """
    è¿è¡Œå®Œæ•´ä¸‰æ–¹æ¡ˆå¯¹æ¯”å®éªŒ
    """
    print("="*80)
    print("ä¸‰æ–¹æ¡ˆç»Ÿä¸€OODæ£€æµ‹å¯¹æ¯”å®éªŒ")
    print("="*80)

    results = {
        'HMCEN-C': {'auroc': [], 'fpr95': [], 'aupr': []},
        'C4-TDA-beta1': {'auroc': [], 'fpr95': [], 'aupr': []},
        'C4-TDA-h_beta1': {'auroc': [], 'fpr95': [], 'aupr': []},
        'C4-TDA-calibrated': {'auroc': [], 'fpr95': [], 'aupr': []},
        'CP-ABR++': {'auroc': [], 'fpr95': [], 'aupr': []}
    }

    for seed_idx, seed in enumerate(seeds):
        print(f"\n{'='*80}")
        print(f"è¿è¡Œ Seed {seed} ({seed_idx+1}/{len(seeds)})")
        print(f"{'='*80}")

        # åŠ è½½æ•°æ®
        np.random.seed(seed)
        data = load_clinc150_for_ood(n_samples=2000, ood_ratio=0.3)
        y_true = data.ood_labels.numpy()

        # æ–¹æ¡ˆ1: HMCEN-C
        print("\n1. è®­ç»ƒHMCEN-C...")
        ood_scores_hmcen = train_hmcen(data, seed=seed)

        auroc = roc_auc_score(y_true, ood_scores_hmcen)
        fpr95 = compute_fpr95(y_true, ood_scores_hmcen)
        aupr = average_precision_score(y_true, ood_scores_hmcen)

        results['HMCEN-C']['auroc'].append(auroc)
        results['HMCEN-C']['fpr95'].append(fpr95)
        results['HMCEN-C']['aupr'].append(aupr)

        print(f"  AUROC: {auroc:.4f}, FPR95: {fpr95:.4f}, AUPR: {aupr:.4f}")

        # æ–¹æ¡ˆ2: C4-TDA
        print("\n2. C4-TDA-OOD...")
        c4tda_scores = c4tda_ood_detection(data, use_calibration=True)

        for variant_name, scores in c4tda_scores.items():
            auroc = roc_auc_score(y_true, scores)
            fpr95 = compute_fpr95(y_true, scores)
            aupr = average_precision_score(y_true, scores)

            key = f'C4-TDA-{variant_name}'
            results[key]['auroc'].append(auroc)
            results[key]['fpr95'].append(fpr95)
            results[key]['aupr'].append(aupr)

            print(f"  {variant_name}: AUROC={auroc:.4f}, FPR95={fpr95:.4f}, AUPR={aupr:.4f}")

        # æ–¹æ¡ˆ3: CP-ABR++
        print("\n3. è®­ç»ƒCP-ABR++...")
        ood_scores_cp = train_cp_abr(data, seed=seed)

        auroc = roc_auc_score(y_true, ood_scores_cp)
        fpr95 = compute_fpr95(y_true, ood_scores_cp)
        aupr = average_precision_score(y_true, ood_scores_cp)

        results['CP-ABR++']['auroc'].append(auroc)
        results['CP-ABR++']['fpr95'].append(fpr95)
        results['CP-ABR++']['aupr'].append(aupr)

        print(f"  AUROC: {auroc:.4f}, FPR95: {fpr95:.4f}, AUPR: {aupr:.4f}")

    # ç»Ÿè®¡æ±‡æ€»
    print("\n" + "="*80)
    print("æœ€ç»ˆç»“æœæ±‡æ€»ï¼ˆå‡å€¼ Â± æ ‡å‡†å·®ï¼‰")
    print("="*80)

    summary_table = []
    for method, metrics in results.items():
        auroc_mean = np.mean(metrics['auroc'])
        auroc_std = np.std(metrics['auroc'])
        fpr95_mean = np.mean(metrics['fpr95'])
        fpr95_std = np.std(metrics['fpr95'])
        aupr_mean = np.mean(metrics['aupr'])
        aupr_std = np.std(metrics['aupr'])

        summary_table.append({
            'method': method,
            'auroc': f"{auroc_mean:.4f}Â±{auroc_std:.4f}",
            'fpr95': f"{fpr95_mean:.4f}Â±{fpr95_std:.4f}",
            'aupr': f"{aupr_mean:.4f}Â±{aupr_std:.4f}",
            'auroc_mean': auroc_mean,
            'auroc_std': auroc_std
        })

    # æ‰“å°è¡¨æ ¼
    print(f"\n{'æ–¹æ³•':<25} {'AUROC':<20} {'FPR95':<20} {'AUPR':<20}")
    print("-"*85)
    for row in summary_table:
        print(f"{row['method']:<25} {row['auroc']:<20} {row['fpr95']:<20} {row['aupr']:<20}")

    # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    print("\n" + "="*80)
    print("ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆvsæœ€ä½³æ–¹æ³•ï¼‰")
    print("="*80)

    best_method = max(summary_table, key=lambda x: x['auroc_mean'])['method']
    best_aurocs = results[best_method]['auroc']

    print(f"\næœ€ä½³æ–¹æ³•: {best_method} (AUROC={np.mean(best_aurocs):.4f})")
    print("\nT-testç»“æœ:")

    for method in results.keys():
        if method != best_method:
            aurocs = results[method]['auroc']
            t_stat, p_value = stats.ttest_rel(best_aurocs, aurocs)

            sig_symbol = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"

            print(f"  {best_method} vs {method}: p={p_value:.4f} {sig_symbol}")

    # å¯è§†åŒ–
    visualize_results(results, summary_table)

    return results, summary_table

def visualize_results(results, summary_table):
    """å¯è§†åŒ–å®éªŒç»“æœ"""
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    methods = list(results.keys())
    colors = plt.cm.Set3(np.linspace(0, 1, len(methods)))

    # AUROCå¯¹æ¯”
    aurocs = [results[m]['auroc'] for m in methods]
    bp1 = axes[0].boxplot(aurocs, labels=methods, patch_artist=True)
    for patch, color in zip(bp1['boxes'], colors):
        patch.set_facecolor(color)
    axes[0].set_ylabel('AUROC', fontsize=12)
    axes[0].set_title('OODæ£€æµ‹æ€§èƒ½å¯¹æ¯” (AUROC)', fontsize=14)
    axes[0].grid(True, alpha=0.3)
    axes[0].tick_params(axis='x', rotation=45)

    # FPR95å¯¹æ¯”
    fpr95s = [results[m]['fpr95'] for m in methods]
    bp2 = axes[1].boxplot(fpr95s, labels=methods, patch_artist=True)
    for patch, color in zip(bp2['boxes'], colors):
        patch.set_facecolor(color)
    axes[1].set_ylabel('FPR@95%TPR', fontsize=12)
    axes[1].set_title('è¯¯æŠ¥ç‡å¯¹æ¯” (è¶Šä½è¶Šå¥½)', fontsize=14)
    axes[1].grid(True, alpha=0.3)
    axes[1].tick_params(axis='x', rotation=45)

    # AUPRå¯¹æ¯”
    auprs = [results[m]['aupr'] for m in methods]
    bp3 = axes[2].boxplot(auprs, labels=methods, patch_artist=True)
    for patch, color in zip(bp3['boxes'], colors):
        patch.set_facecolor(color)
    axes[2].set_ylabel('AUPR', fontsize=12)
    axes[2].set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿ä¸‹é¢ç§¯', fontsize=14)
    axes[2].grid(True, alpha=0.3)
    axes[2].tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.savefig('three_way_comparison.png', dpi=300, bbox_inches='tight')
    print(f"\nå¯è§†åŒ–ç»“æœå·²ä¿å­˜: three_way_comparison.png")

# ==================== æ‰§è¡Œ ====================

if __name__ == '__main__':
    results, summary = run_experiment(seeds=[42, 2024, 2025])

    print("\n" + "="*80)
    print("å®éªŒå®Œæˆï¼")
    print("="*80)

    # å†³ç­–å»ºè®®
    best_row = max(summary, key=lambda x: x['auroc_mean'])
    best_method = best_row['method']
    best_auroc = best_row['auroc_mean']

    print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_method}")
    print(f"   AUROC: {best_auroc:.4f}")

    # å¯¹æ¯”åˆ†æ
    hmcen_auroc = next(r['auroc_mean'] for r in summary if r['method'] == 'HMCEN-C')
    c4tda_best_auroc = max([r['auroc_mean'] for r in summary if 'C4-TDA' in r['method']])
    cp_auroc = next(r['auroc_mean'] for r in summary if r['method'] == 'CP-ABR++')

    print(f"\nå…³é”®å¯¹æ¯”:")
    print(f"  HMCEN-C:          {hmcen_auroc:.4f}")
    print(f"  C4-TDA (æœ€ä½³):    {c4tda_best_auroc:.4f}")
    print(f"  CP-ABR++:        {cp_auroc:.4f}")

    # å†³ç­–å»ºè®®
    print(f"\nğŸ’¡ å†³ç­–å»ºè®®:")

    if hmcen_auroc > c4tda_best_auroc + 0.03:
        print("  âœ… HMCENæ˜¾è‘—ä¼˜äºC4-TDAï¼ˆ+3%ä»¥ä¸Šï¼‰")
        print("  â†’ æ¨è: è€ƒè™‘HMCEN-Liteï¼ˆå¦‚æœæ—¶é—´å…è®¸ï¼‰")
    elif abs(hmcen_auroc - c4tda_best_auroc) < 0.02:
        print("  âš ï¸ HMCENå’ŒC4-TDAæ€§èƒ½ç›¸å½“ï¼ˆÂ±2%ï¼‰")
        print("  â†’ æ¨è: ä¼˜å…ˆC4-TDAï¼ˆæ—¶é—´æ›´çŸ­ï¼Œ3-4æœˆ vs 10-12æœˆï¼‰")
    else:
        print("  âŒ C4-TDAä¼˜äºæˆ–æ¥è¿‘HMCEN")
        print("  â†’ æ¨è: C4-TDAä¸»è½¨")

    # Vanilla GNNåŸºçº¿ï¼ˆéšå¼è®¡ç®—ï¼‰
    # æ³¨æ„: å½“å‰æ²¡æœ‰ç‹¬ç«‹çš„Vanilla GNNç»“æœï¼Œä½†å¯ä»¥ä»æ¶ˆèå®éªŒè·å¾—
    print(f"\nâš ï¸ é‡è¦æé†’:")
    print(f"  - è¿è¡ŒPrompt 4 (HMCENæ¶ˆèå®éªŒ)ä»¥è·å¾—vs Vanilla GNNçš„å¯¹æ¯”")
    print(f"  - è¿™å°†ç¡®è®¤HMCENçš„ä¼˜åŠ¿æ˜¯æ¥è‡ªæ¶æ„è¿˜æ˜¯ä»…ä»…æ˜¯'è®­ç»ƒäº†åˆ†ç±»å™¨'")
