#!/usr/bin/env python3
"""
HT-GIBå‡è®¾éªŒè¯ - DocREDæ•°æ®é›†
äº¤å‰éªŒè¯ï¼šå¼‚é…æ€§ vs å™ªå£°ç‡
"""

import json
import numpy as np
from collections import defaultdict
import random
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("HT-GIB å‡è®¾éªŒè¯ - DocREDæ•°æ®é›†")
print("=" * 70)

# ============================================================
# A. åŠ è½½DocREDæ•°æ®
# ============================================================
print("\n[A] åŠ è½½DocREDæ•°æ®...")

with open('docred_data/DocRED/train_annotated.json', 'r') as f:
    docred_data = json.load(f)

print(f"    æ–‡æ¡£æ•°: {len(docred_data)}")

# å±•å¼€ä¸ºå®ä½“å¯¹æ•°æ®
random.seed(42)

entity_sentences = defaultdict(list)
entity_noise_stats = defaultdict(lambda: {'total': 0, 'noise': 0})
entity_pairs_data = []

total_positive = 0
total_negative = 0

for doc in docred_data:
    # è·å–æ–‡æ¡£æ–‡æœ¬
    sents = doc['sents']
    full_text = ' '.join([' '.join(sent) for sent in sents])

    # è·å–å®ä½“åç§°
    entities = []
    for vertex in doc['vertexSet']:
        # å–ç¬¬ä¸€ä¸ªmentionçš„name
        name = vertex[0]['name']
        entities.append(name)
        entity_sentences[name].append(full_text)

    # è·å–æ­£ä¾‹å…³ç³»ï¼ˆæœ‰æ ‡æ³¨çš„å®ä½“å¯¹ï¼‰
    labels = doc.get('labels', [])
    labeled_pairs = set()

    for label in labels:
        h_idx = label['h']
        t_idx = label['t']
        labeled_pairs.add((h_idx, t_idx))

        h_name = entities[h_idx]
        t_name = entities[t_idx]

        # è¿™æ˜¯æ­£ä¾‹ï¼ˆæœ‰å…³ç³»ï¼‰
        entity_noise_stats[h_name]['total'] += 1
        entity_noise_stats[t_name]['total'] += 1
        # ä¸æ˜¯å™ªå£°
        total_positive += 1

    # è´Ÿä¾‹ï¼šæœªæ ‡æ³¨çš„å®ä½“å¯¹è§†ä¸ºNAï¼ˆå™ªå£°ï¼‰
    n_entities = len(entities)
    for i in range(n_entities):
        for j in range(n_entities):
            if i != j and (i, j) not in labeled_pairs:
                h_name = entities[i]
                t_name = entities[j]

                entity_noise_stats[h_name]['total'] += 1
                entity_noise_stats[t_name]['total'] += 1
                entity_noise_stats[h_name]['noise'] += 1
                entity_noise_stats[t_name]['noise'] += 1
                total_negative += 1

print(f"    å®ä½“æ•°: {len(entity_sentences)}")
print(f"    æ­£ä¾‹(æœ‰å…³ç³»): {total_positive}")
print(f"    è´Ÿä¾‹(NA): {total_negative}")
print(f"    NAæ¯”ä¾‹: {100*total_negative/(total_positive+total_negative):.1f}%")

# ============================================================
# B. æ„å»ºå®ä½“å…±ç°å›¾å¹¶è®¡ç®—å¼‚é…æ€§
# ============================================================
print("\n[B] æ„å»ºå®ä½“å…±ç°å›¾...")

import networkx as nx

G = nx.Graph()

for doc in docred_data:
    entities = [v[0]['name'] for v in doc['vertexSet']]
    for i, e1 in enumerate(entities):
        G.add_node(e1)
        for j, e2 in enumerate(entities):
            if i < j:
                G.add_edge(e1, e2)

print(f"    èŠ‚ç‚¹æ•°: {G.number_of_nodes()}")
print(f"    è¾¹æ•°: {G.number_of_edges()}")

# ============================================================
# C. è®¡ç®—å®ä½“åµŒå…¥å’Œå¼‚é…æ€§
# ============================================================
print("\n[C] è®¡ç®—å®ä½“åµŒå…¥...")

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('all-MiniLM-L6-v2')

# è¿‡æ»¤ï¼šåªä¿ç•™æœ‰è¶³å¤Ÿæ•°æ®çš„å®ä½“
valid_entities = [e for e in entity_sentences.keys()
                  if len(entity_sentences[e]) >= 1
                  and entity_noise_stats[e]['total'] >= 2
                  and e in G.nodes()]

print(f"    æœ‰æ•ˆå®ä½“æ•°: {len(valid_entities)}")

# è®¡ç®—åµŒå…¥
entity_embeddings = {}
for idx, entity in enumerate(valid_entities):
    sentences = entity_sentences[entity][:5]  # æœ€å¤š5ä¸ªæ–‡æ¡£
    if sentences:
        embs = model.encode(sentences, show_progress_bar=False)
        entity_embeddings[entity] = np.mean(embs, axis=0)

    if (idx + 1) % 1000 == 0:
        print(f"    å·²å¤„ç† {idx + 1}/{len(valid_entities)}")

print(f"    åµŒå…¥æ•°: {len(entity_embeddings)}")

# è®¡ç®—å¼‚é…æ€§
print("\n[D] è®¡ç®—å¼‚é…æ€§...")

heterophily_scores = {}

for node in entity_embeddings.keys():
    neighbors = list(G.neighbors(node))
    neighbor_embs = [entity_embeddings[n] for n in neighbors if n in entity_embeddings]

    if len(neighbor_embs) == 0:
        continue

    node_emb = entity_embeddings[node].reshape(1, -1)
    neighbor_matrix = np.array(neighbor_embs)
    similarities = cosine_similarity(node_emb, neighbor_matrix)[0]

    h_v = 1.0 - np.mean(similarities)
    heterophily_scores[node] = h_v

print(f"    å¼‚é…æ€§èŠ‚ç‚¹æ•°: {len(heterophily_scores)}")

# ============================================================
# E. è®¡ç®—å™ªå£°ç‡
# ============================================================
print("\n[E] è®¡ç®—å™ªå£°ç‡...")

noise_rates = {}
for entity in heterophily_scores.keys():
    stats = entity_noise_stats[entity]
    if stats['total'] > 0:
        noise_rates[entity] = stats['noise'] / stats['total']

print(f"    å™ªå£°ç‡èŠ‚ç‚¹æ•°: {len(noise_rates)}")

# ============================================================
# F. ç»Ÿè®¡åˆ†æ
# ============================================================
print("\n[F] ç»Ÿè®¡åˆ†æ...")

import scipy.stats as stats

common_entities = set(heterophily_scores.keys()) & set(noise_rates.keys())
print(f"    åˆ†æèŠ‚ç‚¹æ•°: {len(common_entities)}")

h_values = np.array([heterophily_scores[e] for e in common_entities])
n_values = np.array([noise_rates[e] for e in common_entities])

print(f"    å¼‚é…æ€§èŒƒå›´: [{h_values.min():.3f}, {h_values.max():.3f}]")
print(f"    å™ªå£°ç‡èŒƒå›´: [{n_values.min():.3f}, {n_values.max():.3f}]")
print(f"    å¹³å‡å¼‚é…æ€§: {h_values.mean():.3f}")
print(f"    å¹³å‡å™ªå£°ç‡: {n_values.mean():.3f}")

# Pearsonç›¸å…³
r_pearson, p_pearson = stats.pearsonr(h_values, n_values)

# Spearmanç›¸å…³
r_spearman, p_spearman = stats.spearmanr(h_values, n_values)

# Cohen's d
q1 = np.percentile(h_values, 25)
q4 = np.percentile(h_values, 75)

low_noise = n_values[h_values <= q1]
high_noise = n_values[h_values >= q4]

mean_low = np.mean(low_noise)
mean_high = np.mean(high_noise)
std_low = np.std(low_noise, ddof=1)
std_high = np.std(high_noise, ddof=1)
n_low = len(low_noise)
n_high = len(high_noise)

pooled_std = np.sqrt(((n_low-1)*std_low**2 + (n_high-1)*std_high**2) / (n_low + n_high - 2))
cohens_d = (mean_high - mean_low) / pooled_std if pooled_std > 0 else 0

# ============================================================
# G. è¾“å‡ºç»“æœ
# ============================================================
print("\n" + "=" * 70)
print("DocRED éªŒè¯ç»“æœ")
print("=" * 70)

print(f"\næ•°æ®ç»Ÿè®¡:")
print(f"  æ–‡æ¡£æ•°: {len(docred_data)}")
print(f"  åˆ†æå®ä½“æ•°: {len(common_entities)}")
print(f"  NAæ¯”ä¾‹: {100*total_negative/(total_positive+total_negative):.1f}%")
print(f"  å¹³å‡å¼‚é…æ€§: {h_values.mean():.3f}")
print(f"  å¹³å‡å™ªå£°ç‡: {n_values.mean():.3f}")

print(f"\nç›¸å…³æ€§åˆ†æ:")
print(f"  Pearson r  = {r_pearson:+.4f} (p={p_pearson:.2e})")
print(f"  Spearman Ï = {r_spearman:+.4f} (p={p_spearman:.2e})")
print(f"  Cohen's d  = {cohens_d:+.4f}  â† å…³é”®æŒ‡æ ‡ï¼")

print(f"\nQuartileåˆ†æ:")
print(f"  Q1 (ä½å¼‚é…25%) å™ªå£°ç‡: {mean_low:.4f} (n={n_low})")
print(f"  Q4 (é«˜å¼‚é…25%) å™ªå£°ç‡: {mean_high:.4f} (n={n_high})")
print(f"  å·®å¼‚ (Q4-Q1): {mean_high - mean_low:+.4f}")

# ============================================================
# H. ä¸NYT10å¯¹æ¯”
# ============================================================
print("\n" + "=" * 70)
print("NYT10 vs DocRED å¯¹æ¯”")
print("=" * 70)

print("\næŒ‡æ ‡           NYT10-Train    NYT10-Test     DocRED")
print("-" * 70)
print(f"Cohen's d      -0.6044        -0.2860        {cohens_d:+.4f}")
print(f"Pearson r      -0.2321        -0.1218        {r_pearson:+.4f}")
print(f"Spearman Ï     -0.2722        -0.1900        {r_spearman:+.4f}")
print("-" * 70)

# ============================================================
# I. æœ€ç»ˆç»“è®º
# ============================================================
print("\n" + "=" * 70)
print("æœ€ç»ˆç»“è®º")
print("=" * 70)

# åˆ¤æ–­DocREDç»“æœ
if cohens_d > 0.3:
    docred_result = "æ­£ç›¸å…³"
    print(f"\nğŸ” DocREDç»“æœ: Cohen's d = {cohens_d:+.4f} > 0.3")
    print("   â†’ å¼‚é…æ€§ä¸å™ªå£°ç‡æ­£ç›¸å…³")
elif cohens_d < -0.3:
    docred_result = "è´Ÿç›¸å…³"
    print(f"\nğŸ” DocREDç»“æœ: Cohen's d = {cohens_d:+.4f} < -0.3")
    print("   â†’ å¼‚é…æ€§ä¸å™ªå£°ç‡è´Ÿç›¸å…³")
else:
    docred_result = "æ— æ˜¾è‘—ç›¸å…³"
    print(f"\nğŸ” DocREDç»“æœ: |Cohen's d| = {abs(cohens_d):.4f} < 0.3")
    print("   â†’ æ— æ˜¾è‘—ç›¸å…³æ€§")

# ç»¼åˆåˆ¤æ–­
print("\n" + "-" * 70)
print("ç»¼åˆåˆ¤æ–­:")
print("-" * 70)

nyt_negative = True  # NYT10è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½æ˜¯è´Ÿç›¸å…³

if cohens_d > 0.3:
    # Case 1: DocREDæ­£ç›¸å…³ï¼ŒNYT10è´Ÿç›¸å…³
    print("\nâš ï¸ Case 1: ç»“æœä¸ä¸€è‡´ï¼")
    print()
    print("NYT10: æ˜¾è‘—è´Ÿç›¸å…³ (Cohen's d â‰ˆ -0.6)")
    print(f"DocRED: æ˜¾è‘—æ­£ç›¸å…³ (Cohen's d = {cohens_d:+.4f})")
    print()
    print("å¯èƒ½åŸå› :")
    print("1. æ•°æ®é›†ç‰¹æ€§ä¸åŒï¼ˆæ–°é—» vs ç™¾ç§‘ï¼‰")
    print("2. è¿œç¨‹ç›‘ç£ vs äººå·¥æ ‡æ³¨")
    print("3. å¥å­çº§ vs æ–‡æ¡£çº§")
    print()
    print("å»ºè®®:")
    print("â€¢ HT-GIBå¯èƒ½åªé€‚ç”¨ç‰¹å®šåœºæ™¯")
    print("â€¢ éœ€è¦æ·±å…¥åˆ†ææ•°æ®é›†å·®å¼‚")
    print("â€¢ é£é™©ï¼šä¸­ç­‰ï¼ˆéœ€è¦é¢å¤–ç ”ç©¶ï¼‰")

elif cohens_d < -0.3:
    # Case 2: ä¸¤ä¸ªæ•°æ®é›†éƒ½è´Ÿç›¸å…³
    print("\nâœ… Case 2: ç»“æœä¸€è‡´ï¼")
    print()
    print("NYT10-Train: Cohen's d = -0.6044 (è´Ÿç›¸å…³)")
    print("NYT10-Test:  Cohen's d = -0.2860 (è´Ÿç›¸å…³)")
    print(f"DocRED:      Cohen's d = {cohens_d:+.4f} (è´Ÿç›¸å…³)")
    print()
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚  ä¸‰ä¸ªæ•°æ®é›†ç»“æœä¸€è‡´ï¼šå¼‚é…æ€§ä¸å™ªå£°ç‡å‘ˆè´Ÿç›¸å…³            â”‚")
    print("â”‚  HT-GIBæ ¸å¿ƒå‡è®¾å½»åº•å¤±è´¥ï¼                              â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()
    print("ğŸ“‹ å¼ºçƒˆå»ºè®®:")
    print("   1. ç«‹å³æ”¾å¼ƒ HT-GIB æ–¹æ¡ˆ")
    print("   2. åˆ‡æ¢åˆ°å¤‡é€‰æ–¹æ¡ˆ:")
    print("      â€¢ HDCL-RE (å¼‚æ„åŒå¡”å¯¹æ¯”å­¦ä¹ )")
    print("      â€¢ æ ‡å‡†å¯¹æ¯”å­¦ä¹ å»å™ª")
    print("      â€¢ ä¸ç¡®å®šæ€§å¼•å¯¼æ³¨æ„åŠ›")
    print()
    print("â±ï¸ éªŒè¯æ—¶é—´: çº¦3å°æ—¶")
    print("ğŸ’° åŠæ—¶æ­¢æŸï¼Œé¿å…æµªè´¹4å‘¨å®æ–½æ—¶é—´")

else:
    # Case 3: DocREDæ— æ˜¾è‘—ç›¸å…³
    print("\nâš ï¸ Case 3: DocREDæ— æ˜¾è‘—ç›¸å…³")
    print()
    print("NYT10: æ˜¾è‘—è´Ÿç›¸å…³")
    print(f"DocRED: æ— æ˜¾è‘—ç›¸å…³ (Cohen's d = {cohens_d:+.4f})")
    print()
    print("ç»“è®º:")
    print("â€¢ NYT10å·²ç¡®è®¤å‡è®¾å¤±è´¥ï¼ˆè´Ÿç›¸å…³ï¼‰")
    print("â€¢ DocREDæ²¡æœ‰æ”¯æŒè¯æ®")
    print("â€¢ è‡³å°‘ä¸€ä¸ªä¸»æµæ•°æ®é›†éªŒè¯å¤±è´¥")
    print()
    print("å»ºè®®: å€¾å‘äºæ”¾å¼ƒHT-GIB")
    print("é£é™©: é«˜ï¼ˆæ— è¶³å¤Ÿè¯æ®æ”¯æŒå‡è®¾ï¼‰")

print("\n" + "=" * 70)
