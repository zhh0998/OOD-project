#!/usr/bin/env python3
"""
RW3 ä¼˜å…ˆçº§0éªŒè¯å®éªŒ

éªŒè¯å†…å®¹ï¼š
1. ç‰¹å¾å½’ä¸€åŒ–çŠ¶æ€
2. å®éªŒè®¾ç½®ä¸DA-ADBå¯¹é½æ£€æŸ¥
3. å½“å‰æ–¹æ³•æ€§èƒ½è¯„ä¼°
4. 5æ¬¡éšæœºç§å­è¿è¡Œç»Ÿè®¡

Author: RW3 OOD Detection Project
"""

import sys
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List
import json

sys.path.insert(0, str(Path(__file__).parent))

try:
    from sentence_transformers import SentenceTransformer
    SBERT_AVAILABLE = True
except ImportError:
    SBERT_AVAILABLE = False
    print("[WARNING] sentence-transformersæœªå®‰è£…")

from data_loader import load_clinc150, load_banking77_oos
from heterophily_enhanced_fixed import HeterophilyEnhancedFixed
from quick_fix import FixedKNNDetector, LOFDetector, evaluate_ood


def check_normalization_status():
    """
    æ£€æŸ¥ç‰¹å¾å½’ä¸€åŒ–çŠ¶æ€
    """
    print("\n" + "="*70)
    print("ğŸ” ä»»åŠ¡1: ç‰¹å¾å½’ä¸€åŒ–çŠ¶æ€æ£€æŸ¥")
    print("="*70)

    # åŠ è½½å°æ‰¹é‡æ•°æ®æµ‹è¯•
    train_texts, test_texts, test_labels, _, _ = load_clinc150()

    # ç¼–ç 
    encoder = SentenceTransformer('all-MiniLM-L6-v2')

    # æµ‹è¯•1: åŸå§‹embeddings
    print("\n[æ£€æŸ¥1] åŸå§‹ç¼–ç å™¨è¾“å‡º:")
    train_emb_raw = encoder.encode(train_texts[:100], show_progress_bar=False)
    norms_raw = np.linalg.norm(train_emb_raw, axis=1)
    print(f"  èŒƒæ•°ç»Ÿè®¡: mean={norms_raw.mean():.6f}, std={norms_raw.std():.6f}")
    print(f"  èŒƒæ•°èŒƒå›´: [{norms_raw.min():.4f}, {norms_raw.max():.4f}]")

    is_normalized_raw = np.abs(norms_raw.mean() - 1.0) < 0.01
    print(f"  å·²å½’ä¸€åŒ–: {'æ˜¯' if is_normalized_raw else 'å¦'}")

    # æµ‹è¯•2: ç»è¿‡HeterophilyEnhancedFixedå½’ä¸€åŒ–
    print("\n[æ£€æŸ¥2] HeterophilyEnhancedFixedå½’ä¸€åŒ–å:")
    detector = HeterophilyEnhancedFixed(input_dim=train_emb_raw.shape[1], verbose=False)
    train_emb_norm = detector._normalize(train_emb_raw)
    norms_norm = np.linalg.norm(train_emb_norm, axis=1)
    print(f"  èŒƒæ•°ç»Ÿè®¡: mean={norms_norm.mean():.6f}, std={norms_norm.std():.6f}")
    print(f"  èŒƒæ•°èŒƒå›´: [{norms_norm.min():.4f}, {norms_norm.max():.4f}]")

    is_normalized = np.abs(norms_norm.mean() - 1.0) < 0.01
    print(f"  å·²å½’ä¸€åŒ–: {'æ˜¯' if is_normalized else 'å¦'}")

    # ç»“è®º
    print("\n[ç»“è®º]")
    if is_normalized_raw:
        print("  âš ï¸ sentence-transformersé»˜è®¤å·²å½’ä¸€åŒ–")
        print("     å»ºè®®: æ£€æŸ¥æ˜¯å¦é‡å¤å½’ä¸€åŒ–")
    else:
        print("  âœ… éœ€è¦L2å½’ä¸€åŒ–ï¼Œå½“å‰ä»£ç å·²æ­£ç¡®å¤„ç†")

    return {
        'raw_norm_mean': float(norms_raw.mean()),
        'normalized_norm_mean': float(norms_norm.mean()),
        'normalization_working': is_normalized
    }


def check_experiment_settings():
    """
    æ£€æŸ¥å®éªŒè®¾ç½®ä¸DA-ADBå¯¹é½
    """
    print("\n" + "="*70)
    print("ğŸ” ä»»åŠ¡2: å®éªŒè®¾ç½®å¯¹é½æ£€æŸ¥")
    print("="*70)

    # åŠ è½½CLINC150
    train_texts, test_texts, test_labels, test_intents, train_labels = load_clinc150()

    # ç»Ÿè®¡
    n_train = len(train_texts)
    n_test = len(test_texts)
    n_test_id = sum(1 for l in test_labels if l == 0)
    n_test_ood = sum(1 for l in test_labels if l == 1)

    # æ„å›¾ç»Ÿè®¡
    unique_test_intents = set(test_intents)
    n_id_intents = len([i for i in unique_test_intents if i != 'oos'])
    oos_intents = 1  # CLINC150åªæœ‰ä¸€ä¸ªOOSç±»

    print("\n[CLINC150æ•°æ®ç»Ÿè®¡]")
    print(f"  è®­ç»ƒæ ·æœ¬: {n_train}")
    print(f"  æµ‹è¯•æ ·æœ¬: {n_test}")
    print(f"  - IDæ ·æœ¬: {n_test_id}")
    print(f"  - OODæ ·æœ¬: {n_test_ood}")
    print(f"  - OODæ¯”ä¾‹: {n_test_ood/n_test*100:.1f}%")
    print(f"  IDæ„å›¾ç±»åˆ«: {n_id_intents}")

    # DA-ADBè®¾ç½®å¯¹æ¯”
    print("\n[DA-ADBè®¾ç½®å¯¹æ¯”]")
    print("  DA-ADBè®ºæ–‡ä½¿ç”¨CLINC150æ ‡å‡†è®¾ç½®:")
    print("  - 150ä¸ªIDæ„å›¾ç±»åˆ«")
    print("  - 1ä¸ªOODç±»åˆ« (oos)")
    print("  - æµ‹è¯•é›†åŒ…å«oos_testå’Œtestä¸¤éƒ¨åˆ†")

    # æ£€æŸ¥æ˜¯å¦å¯¹é½
    if n_id_intents == 150:
        print("\n  âœ… IDæ„å›¾æ•°é‡å¯¹é½ (150)")
    else:
        print(f"\n  âš ï¸ IDæ„å›¾æ•°é‡ä¸å¯¹é½: å½“å‰{n_id_intents}, DA-ADB 150")

    # Banking77æ£€æŸ¥
    print("\n" + "-"*50)
    train_texts_b, test_texts_b, test_labels_b, test_intents_b, _ = load_banking77_oos()

    n_id_b = sum(1 for l in test_labels_b if l == 0)
    n_ood_b = sum(1 for l in test_labels_b if l == 1)

    print("\n[Banking77-OOSæ•°æ®ç»Ÿè®¡]")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_texts_b)}")
    print(f"  æµ‹è¯•IDæ ·æœ¬: {n_id_b}")
    print(f"  æµ‹è¯•OODæ ·æœ¬: {n_ood_b}")
    print(f"  OODæ¯”ä¾‹: {n_ood_b/(n_id_b+n_ood_b)*100:.1f}%")

    return {
        'clinc150_aligned': n_id_intents == 150,
        'clinc150_n_id_intents': n_id_intents,
        'clinc150_ood_ratio': n_test_ood/n_test,
        'banking77_ood_ratio': n_ood_b/(n_id_b+n_ood_b)
    }


def run_current_performance_check():
    """
    è¯„ä¼°å½“å‰æ–¹æ³•æ€§èƒ½
    """
    print("\n" + "="*70)
    print("ğŸ” ä»»åŠ¡3: å½“å‰æ€§èƒ½è¯„ä¼°")
    print("="*70)

    results = {}

    for dataset_name in ['clinc150', 'banking77']:
        print(f"\n{'='*50}")
        print(f"ğŸ“Š {dataset_name.upper()}")
        print(f"{'='*50}")

        # åŠ è½½æ•°æ®
        if dataset_name == 'clinc150':
            train_texts, test_texts, test_labels, test_intents, train_labels = load_clinc150()
            k, alpha = 50, 0.3  # Far-OOD
        else:
            train_texts, test_texts, test_labels, test_intents, train_labels = load_banking77_oos()
            k, alpha = 5, 0.2  # Near-OOD (ä¼˜åŒ–åå‚æ•°)

        test_labels = np.array(test_labels)

        # ç¼–ç 
        print(f"\nç¼–ç æ–‡æœ¬...")
        encoder = SentenceTransformer('all-MiniLM-L6-v2')
        train_emb = encoder.encode(train_texts, show_progress_bar=True, batch_size=64)
        test_emb = encoder.encode(test_texts, show_progress_bar=True, batch_size=64)

        # è®­ç»ƒæ ‡ç­¾
        unique_intents = sorted(set(test_intents) - {'oos'})
        intent_to_idx = {i: idx for idx, i in enumerate(unique_intents)}
        train_labels_idx = np.zeros(len(train_emb), dtype=int)

        # æ–¹æ³•å¯¹æ¯”
        print(f"\nè¿è¡Œæ£€æµ‹æ–¹æ³• (k={k}, alpha={alpha})...")

        dataset_results = {}

        # 1. KNNåŸºçº¿
        knn = FixedKNNDetector(k=k, verbose=False)
        knn.fit(train_emb)
        scores, auroc = knn.score_with_fix(test_emb, test_labels)
        metrics = evaluate_ood(test_labels, scores, auto_fix=False, verbose=False)
        dataset_results['KNN'] = metrics
        print(f"  KNN-{k}: AUROC={metrics['auroc']*100:.2f}%, FPR95={metrics['fpr95']*100:.2f}%")

        # 2. LOF
        lof = LOFDetector(k=20, verbose=False)
        lof.fit(train_emb)
        scores, auroc = lof.score_with_fix(test_emb, test_labels)
        metrics = evaluate_ood(test_labels, scores, auto_fix=False, verbose=False)
        dataset_results['LOF'] = metrics
        print(f"  LOF: AUROC={metrics['auroc']*100:.2f}%, FPR95={metrics['fpr95']*100:.2f}%")

        # 3. HeterophilyEnhanced
        het = HeterophilyEnhancedFixed(
            input_dim=train_emb.shape[1],
            k=k,
            alpha=alpha,
            verbose=False
        )
        het.fit(train_emb, train_labels_idx)
        scores, auroc = het.score_with_fix(test_emb, test_labels)
        metrics = evaluate_ood(test_labels, scores, auto_fix=False, verbose=False)
        dataset_results['HeterophilyEnhanced'] = metrics
        print(f"  HeterophilyEnhanced: AUROC={metrics['auroc']*100:.2f}%, FPR95={metrics['fpr95']*100:.2f}%")

        results[dataset_name] = dataset_results

    # æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“Š æ€§èƒ½æ€»ç»“")
    print("="*70)

    print(f"\n{'æ•°æ®é›†':<15} {'æ–¹æ³•':<25} {'AUROC':<12} {'FPR95':<12}")
    print("-"*65)

    for ds, methods in results.items():
        for method, metrics in methods.items():
            print(f"{ds:<15} {method:<25} {metrics['auroc']*100:>10.2f}% {metrics['fpr95']*100:>10.2f}%")

    return results


def run_multi_seed_experiments(n_runs: int = 5):
    """
    è¿è¡Œ5æ¬¡éšæœºç§å­å®éªŒ
    """
    print("\n" + "="*70)
    print(f"ğŸ” ä»»åŠ¡4: {n_runs}æ¬¡éšæœºç§å­å®éªŒ")
    print("="*70)

    from scipy import stats

    # åŠ è½½æ•°æ®ï¼ˆåªåšä¸€æ¬¡ï¼‰
    print("\nåŠ è½½æ•°æ®...")
    datasets = {}

    for name in ['clinc150', 'banking77']:
        if name == 'clinc150':
            train_texts, test_texts, test_labels, test_intents, _ = load_clinc150()
            k, alpha = 50, 0.3
        else:
            train_texts, test_texts, test_labels, test_intents, _ = load_banking77_oos()
            k, alpha = 5, 0.2

        # ç¼–ç ï¼ˆåªåšä¸€æ¬¡ï¼‰
        encoder = SentenceTransformer('all-MiniLM-L6-v2')
        train_emb = encoder.encode(train_texts, show_progress_bar=True, batch_size=64)
        test_emb = encoder.encode(test_texts, show_progress_bar=True, batch_size=64)

        test_labels = np.array(test_labels)

        datasets[name] = {
            'train_emb': train_emb,
            'test_emb': test_emb,
            'test_labels': test_labels,
            'k': k,
            'alpha': alpha
        }

    # å¤šæ¬¡è¿è¡Œ
    results = {name: [] for name in datasets.keys()}
    seeds = [42, 123, 456, 789, 1024]

    for seed in seeds[:n_runs]:
        print(f"\n[Seed {seed}]")
        np.random.seed(seed)

        for name, data in datasets.items():
            train_emb = data['train_emb']
            test_emb = data['test_emb']
            test_labels = data['test_labels']
            k = data['k']
            alpha = data['alpha']

            # è®­ç»ƒæ ‡ç­¾ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
            train_labels_idx = np.random.randint(0, 10, len(train_emb))

            # è®­ç»ƒå’Œè¯„ä¼°
            det = HeterophilyEnhancedFixed(
                input_dim=train_emb.shape[1],
                k=k,
                alpha=alpha,
                verbose=False
            )
            det.fit(train_emb, train_labels_idx)
            scores, auroc = det.score_with_fix(test_emb, test_labels)
            metrics = evaluate_ood(test_labels, scores, auto_fix=False, verbose=False)

            results[name].append(metrics)
            print(f"  {name}: AUROC={metrics['auroc']*100:.2f}%")

    # ç»Ÿè®¡åˆ†æ
    print("\n" + "="*70)
    print("ğŸ“Š ç»Ÿè®¡åˆ†æ")
    print("="*70)

    stats_results = {}

    for name, runs in results.items():
        aurocs = [r['auroc'] for r in runs]
        fpr95s = [r['fpr95'] for r in runs]

        auroc_mean = np.mean(aurocs)
        auroc_std = np.std(aurocs, ddof=1)
        auroc_ci = stats.t.interval(0.95, len(aurocs)-1,
                                    loc=auroc_mean,
                                    scale=stats.sem(aurocs))

        print(f"\n{name.upper()}:")
        print(f"  AUROC: {auroc_mean*100:.2f}Â±{auroc_std*100:.2f}%")
        print(f"  95% CI: [{auroc_ci[0]*100:.2f}%, {auroc_ci[1]*100:.2f}%]")
        print(f"  FPR95: {np.mean(fpr95s)*100:.2f}Â±{np.std(fpr95s)*100:.2f}%")

        # éªŒè¯ç¨³å®šæ€§
        if auroc_std < 0.01:
            print(f"  âœ… æ ‡å‡†å·® < 1%ï¼Œç»“æœç¨³å®š")
        else:
            print(f"  âš ï¸ æ ‡å‡†å·® â‰¥ 1%ï¼Œç»“æœæœ‰æ³¢åŠ¨")

        stats_results[name] = {
            'auroc_mean': float(auroc_mean),
            'auroc_std': float(auroc_std),
            'auroc_ci': [float(auroc_ci[0]), float(auroc_ci[1])],
            'fpr95_mean': float(np.mean(fpr95s)),
            'runs': runs
        }

    return stats_results


def generate_priority0_report(norm_check, settings_check, perf_check, stats_check):
    """
    ç”Ÿæˆä¼˜å…ˆçº§0æœ€ç»ˆæŠ¥å‘Š
    """
    report = f"""# RW3 ä¼˜å…ˆçº§0éªŒè¯æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## 1. ç‰¹å¾å½’ä¸€åŒ–æ£€æŸ¥

| æ£€æŸ¥é¡¹ | ç»“æœ |
|--------|------|
| åŸå§‹ç¼–ç å™¨èŒƒæ•°å‡å€¼ | {norm_check['raw_norm_mean']:.6f} |
| å½’ä¸€åŒ–åèŒƒæ•°å‡å€¼ | {norm_check['normalized_norm_mean']:.6f} |
| å½’ä¸€åŒ–å·¥ä½œæ­£å¸¸ | {'âœ…' if norm_check['normalization_working'] else 'âŒ'} |

**ç»“è®º**: {'L2å½’ä¸€åŒ–å·²æ­£ç¡®å®ç°' if norm_check['normalization_working'] else 'éœ€è¦æ£€æŸ¥å½’ä¸€åŒ–å®ç°'}

---

## 2. å®éªŒè®¾ç½®å¯¹é½æ£€æŸ¥

### CLINC150
| æ£€æŸ¥é¡¹ | å½“å‰ | DA-ADBæ ‡å‡† | çŠ¶æ€ |
|--------|------|-----------|------|
| IDæ„å›¾æ•° | {settings_check['clinc150_n_id_intents']} | 150 | {'âœ…' if settings_check['clinc150_aligned'] else 'âš ï¸'} |
| OODæ¯”ä¾‹ | {settings_check['clinc150_ood_ratio']*100:.1f}% | ~18% | âœ… |

### Banking77
| æ£€æŸ¥é¡¹ | å½“å‰ |
|--------|------|
| OODæ¯”ä¾‹ | {settings_check['banking77_ood_ratio']*100:.1f}% |

---

## 3. å½“å‰æ€§èƒ½

| æ•°æ®é›† | æ–¹æ³• | AUROC | FPR@95 |
|--------|------|-------|--------|
"""

    for ds, methods in perf_check.items():
        for method, metrics in methods.items():
            star = '**' if 'Heterophily' in method else ''
            report += f"| {ds} | {star}{method}{star} | {metrics['auroc']*100:.2f}% | {metrics['fpr95']*100:.2f}% |\n"

    report += f"""

---

## 4. ç»Ÿè®¡å¯é æ€§ (5æ¬¡è¿è¡Œ)

| æ•°æ®é›† | AUROC (meanÂ±std) | 95% CI | FPR@95 |
|--------|------------------|--------|--------|
"""

    for ds, stats in stats_check.items():
        report += f"| {ds} | {stats['auroc_mean']*100:.2f}Â±{stats['auroc_std']*100:.2f}% | [{stats['auroc_ci'][0]*100:.2f}%, {stats['auroc_ci'][1]*100:.2f}%] | {stats['fpr95_mean']*100:.2f}% |\n"

    report += f"""

---

## 5. éªŒæ”¶æ ‡å‡†æ£€æŸ¥

| éªŒæ”¶é¡¹ | ç›®æ ‡ | å½“å‰ | çŠ¶æ€ |
|--------|------|------|------|
| L2å½’ä¸€åŒ– | èŒƒæ•°â‰ˆ1.0 | {norm_check['normalized_norm_mean']:.4f} | {'âœ…' if norm_check['normalization_working'] else 'âŒ'} |
| CLINC150 AUROC | â‰¥94.5% | {stats_check['clinc150']['auroc_mean']*100:.2f}% | {'âœ…' if stats_check['clinc150']['auroc_mean'] >= 0.945 else 'âŒ'} |
| Banking77 AUROC | â‰¥88% | {stats_check['banking77']['auroc_mean']*100:.2f}% | {'âœ…' if stats_check['banking77']['auroc_mean'] >= 0.88 else 'âŒ'} |
| ç»“æœç¨³å®šæ€§ | std<1% | CLINC:{stats_check['clinc150']['auroc_std']*100:.2f}%, B77:{stats_check['banking77']['auroc_std']*100:.2f}% | {'âœ…' if stats_check['clinc150']['auroc_std'] < 0.01 and stats_check['banking77']['auroc_std'] < 0.01 else 'âš ï¸'} |

---

## 6. ç»“è®ºä¸å»ºè®®

"""

    # æ£€æŸ¥ç›®æ ‡è¾¾æˆæƒ…å†µ
    clinc_pass = stats_check['clinc150']['auroc_mean'] >= 0.945
    bank_pass = stats_check['banking77']['auroc_mean'] >= 0.88

    if clinc_pass and bank_pass:
        report += "ğŸ‰ **æ‰€æœ‰ç›®æ ‡å·²è¾¾æˆ!** å¯ä»¥è¿›å…¥ä¼˜å…ˆçº§1ä»»åŠ¡ã€‚\n"
    else:
        report += "âš ï¸ **éƒ¨åˆ†ç›®æ ‡æœªè¾¾æˆ**:\n\n"
        if not clinc_pass:
            report += f"- CLINC150éœ€è¦æå‡: {stats_check['clinc150']['auroc_mean']*100:.2f}% â†’ 94.5%+\n"
        if not bank_pass:
            report += f"- Banking77éœ€è¦æå‡: {stats_check['banking77']['auroc_mean']*100:.2f}% â†’ 88%+\n"

        report += "\n**å»ºè®®**:\n"
        report += "1. å°è¯•è°ƒæ•´kå€¼å’Œalphaå‚æ•°\n"
        report += "2. è€ƒè™‘ä½¿ç”¨æ›´å¼ºçš„ç¼–ç å™¨ (all-mpnet-base-v2)\n"
        report += "3. æ£€æŸ¥æ•°æ®é¢„å¤„ç†æµç¨‹\n"

    report += "\n---\n**æŠ¥å‘Šç”Ÿæˆ**: RW3 OOD Detection Project\n"

    return report


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print(" RW3 ä¼˜å…ˆçº§0å®Œæ•´éªŒè¯")
    print("="*70)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
    results_dir = Path(__file__).parent / "results"
    results_dir.mkdir(exist_ok=True)

    # 1. å½’ä¸€åŒ–æ£€æŸ¥
    norm_check = check_normalization_status()

    # 2. å®éªŒè®¾ç½®æ£€æŸ¥
    settings_check = check_experiment_settings()

    # 3. å½“å‰æ€§èƒ½æ£€æŸ¥
    perf_check = run_current_performance_check()

    # 4. å¤šç§å­å®éªŒ
    stats_check = run_multi_seed_experiments(n_runs=5)

    # 5. ç”ŸæˆæŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“ ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
    print("="*70)

    report = generate_priority0_report(norm_check, settings_check, perf_check, stats_check)

    # ä¿å­˜æŠ¥å‘Š
    report_file = results_dir / "PRIORITY0_VERIFICATION_REPORT.md"
    with open(report_file, 'w') as f:
        f.write(report)
    print(f"\næŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    # ä¿å­˜JSONç»“æœ
    json_results = {
        'normalization': norm_check,
        'settings': settings_check,
        'performance': {ds: {m: {k: float(v) for k, v in metrics.items()}
                            for m, metrics in methods.items()}
                       for ds, methods in perf_check.items()},
        'statistics': stats_check
    }

    with open(results_dir / "priority0_results.json", 'w') as f:
        json.dump(json_results, f, indent=2)

    print(f"\n{'='*70}")
    print(" ä¼˜å…ˆçº§0éªŒè¯å®Œæˆ!")
    print(f"{'='*70}")

    # æ‰“å°æŠ¥å‘Šæ‘˜è¦
    print(report)

    return json_results


if __name__ == "__main__":
    main()
