#!/usr/bin/env python3
"""
C4-TDAè½¬åŒ–èƒ½åŠ›éªŒè¯å®éªŒ
====================================
éªŒè¯C4-TDAçš„ç†è®ºå‘ç°èƒ½å¦è½¬åŒ–ä¸ºå®ç”¨çš„OODæ£€æµ‹æ–¹æ³•

æ ¸å¿ƒé—®é¢˜: C4-TDAå·²ç»è¯æ˜äº†ç†è®ºå‡è®¾ï¼ˆå¼‚é…æ€§â†”Bettiæ•°ï¼Œd=0.9378ï¼‰ï¼Œ
ä½†è¿™ä¸ªå‘ç°èƒ½å¦è½¬åŒ–ä¸ºå®ç”¨çš„OODæ£€æµ‹æ–¹æ³•ï¼Ÿ

éªŒè¯é‡ç‚¹:
1. ç›´æ¥åº”ç”¨èƒ½åŠ›ï¼ˆæ— éœ€è®­ç»ƒï¼‰
2. è½»é‡æ ¡å‡†æ•ˆæœï¼ˆé€»è¾‘å›å½’ï¼‰
3. ä¸HMCENçš„çœŸå®å·®è·
4. è·¨æ•°æ®é›†ç¨³å®šæ€§
"""

import numpy as np
import torch
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)

# ==================== æ•°æ®åŠ è½½ ====================

def load_clinc150_for_ood(n_samples=2000, ood_ratio=0.3):
    """åŠ è½½CLINC150æ•°æ®ï¼ˆä¸Prompt 1ç›¸åŒï¼‰"""
    from datasets import load_dataset
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.neighbors import NearestNeighbors
    from torch_geometric.data import Data

    print("åŠ è½½CLINC150æ•°æ®é›†...")
    dataset = load_dataset('clinc_oos', 'plus')

    test_data = dataset['test']
    indices = np.random.choice(len(test_data), n_samples, replace=False)
    texts = [test_data[i]['text'] for i in indices]
    labels = [test_data[i]['intent'] for i in indices]

    # TF-IDF
    vectorizer = TfidfVectorizer(max_features=300)
    features = vectorizer.fit_transform(texts).toarray()

    # æ„å»ºå›¾
    k = 20
    knn = NearestNeighbors(n_neighbors=k+1, metric='cosine')
    knn.fit(features)
    distances, indices_knn = knn.kneighbors(features)

    edge_list = []
    for i in range(n_samples):
        for j in range(1, k+1):
            neighbor = indices_knn[i, j]
            edge_list.append([i, neighbor])
            edge_list.append([neighbor, i])

    edge_index = torch.tensor(edge_list, dtype=torch.long).t()

    # OODåˆ’åˆ†
    unique_labels = list(set(labels))
    n_ood_classes = int(len(unique_labels) * ood_ratio)
    ood_classes = np.random.choice(unique_labels, n_ood_classes, replace=False)

    ood_labels = np.array([1 if label in ood_classes else 0 for label in labels])

    data = Data(
        x=torch.tensor(features, dtype=torch.float32),
        edge_index=edge_index,
        ood_labels=torch.tensor(ood_labels, dtype=torch.long)
    )

    print(f"æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬: {n_samples}")
    print(f"  ID: {(ood_labels==0).sum()} ({(ood_labels==0).sum()/n_samples*100:.1f}%)")
    print(f"  OOD: {(ood_labels==1).sum()} ({(ood_labels==1).sum()/n_samples*100:.1f}%)")

    return data

# ==================== æ‹“æ‰‘ç‰¹å¾è®¡ç®— ====================

def compute_betti_numbers_simple(edge_index, num_nodes):
    """è®¡ç®—Bettiæ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    from scipy.sparse import csr_matrix
    from scipy.sparse.csgraph import connected_components

    if edge_index.shape[1] == 0:
        return 0, 0

    adj = csr_matrix(
        (np.ones(edge_index.shape[1]),
         (edge_index[0].numpy(), edge_index[1].numpy())),
        shape=(num_nodes, num_nodes)
    )
    n_components, labels = connected_components(adj, directed=False)
    beta_0 = n_components - 1

    n_edges = edge_index.shape[1] // 2
    n_vertices = num_nodes
    beta_1 = max(0, n_edges - n_vertices + n_components)

    return beta_0, beta_1

def compute_node_betti_numbers(data):
    """è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„Bettiæ•°"""
    num_nodes = data.x.shape[0]
    betti_0_list = []
    betti_1_list = []

    print("è®¡ç®—èŠ‚ç‚¹çº§æ‹“æ‰‘ç‰¹å¾...")
    for node_id in range(num_nodes):
        if (node_id + 1) % 500 == 0:
            print(f"  è¿›åº¦: {node_id+1}/{num_nodes}")

        # æå–ego-graph
        neighbors = data.edge_index[1][data.edge_index[0] == node_id]
        if len(neighbors) == 0:
            betti_0_list.append(0)
            betti_1_list.append(0)
            continue

        subgraph_nodes = torch.cat([torch.tensor([node_id]), neighbors])

        # å­å›¾è¾¹
        mask = (torch.isin(data.edge_index[0], subgraph_nodes) &
                torch.isin(data.edge_index[1], subgraph_nodes))
        subgraph_edges = data.edge_index[:, mask]

        if subgraph_edges.shape[1] == 0:
            betti_0_list.append(0)
            betti_1_list.append(0)
            continue

        # é‡æ–°ç¼–å·
        node_map = {n.item(): i for i, n in enumerate(subgraph_nodes)}
        subgraph_edges_reindexed = torch.tensor([
            [node_map[e[0].item()] for e in subgraph_edges.t()],
            [node_map[e[1].item()] for e in subgraph_edges.t()]
        ])

        # è®¡ç®—Bettiæ•°
        beta_0, beta_1 = compute_betti_numbers_simple(
            subgraph_edges_reindexed,
            len(subgraph_nodes)
        )

        betti_0_list.append(beta_0)
        betti_1_list.append(beta_1)

    return np.array(betti_0_list), np.array(betti_1_list)

def compute_heterophily_pseudo(data):
    """è®¡ç®—ä¼ªå¼‚é…æ€§"""
    num_nodes = data.x.shape[0]
    h_node = torch.zeros(num_nodes)

    for v in range(num_nodes):
        neighbors = data.edge_index[1][data.edge_index[0] == v]
        if len(neighbors) > 0:
            feat_sim = F.cosine_similarity(
                data.x[v].unsqueeze(0),
                data.x[neighbors],
                dim=1
            )
            h_node[v] = 1 - feat_sim.mean()

    return h_node

def compute_clustering_coefficient(data):
    """è®¡ç®—èŠ‚ç‚¹èšç±»ç³»æ•°"""
    num_nodes = data.x.shape[0]
    clustering = np.zeros(num_nodes)

    print("è®¡ç®—èšç±»ç³»æ•°...")
    for node_id in range(num_nodes):
        if (node_id + 1) % 500 == 0:
            print(f"  è¿›åº¦: {node_id+1}/{num_nodes}")

        neighbors = data.edge_index[1][data.edge_index[0] == node_id].numpy()
        k = len(neighbors)

        if k < 2:
            clustering[node_id] = 0
            continue

        # é‚»å±…é—´çš„è¾¹æ•° - ä½¿ç”¨é›†åˆä¼˜åŒ–
        neighbor_set = set(neighbors)
        neighbor_edges = 0

        for n in neighbors:
            n_neighbors = data.edge_index[1][data.edge_index[0] == n].numpy()
            for nn in n_neighbors:
                if nn in neighbor_set and nn > n:  # é¿å…é‡å¤è®¡æ•°
                    neighbor_edges += 1

        # èšç±»ç³»æ•°
        max_edges = k * (k - 1) / 2
        clustering[node_id] = neighbor_edges / max_edges if max_edges > 0 else 0

    return clustering

# ==================== ä¸‰ç§è½¬åŒ–æ–¹æ³• ====================

def method1_direct_beta1(betti_1):
    """æ–¹æ³•1: Î²â‚ç›´æ¥ä½œä¸ºOODåˆ†æ•°"""
    return betti_1

def method2_h_times_beta1(h_node, betti_1):
    """æ–¹æ³•2: h*Î²â‚ï¼ˆç»“åˆå¼‚é…æ€§ï¼‰"""
    return h_node.numpy() * betti_1

def method3_calibrated(data, h_node, betti_0, betti_1):
    """æ–¹æ³•3: è½»é‡æ ¡å‡†ï¼ˆé€»è¾‘å›å½’ï¼‰"""
    from torch_geometric.utils import degree

    # æ„å»ºç‰¹å¾
    degrees = degree(data.edge_index[0], num_nodes=data.x.shape[0]).numpy()

    # é¢å¤–ç‰¹å¾ï¼šèšç±»ç³»æ•°
    clustering = compute_clustering_coefficient(data)

    features = np.column_stack([
        h_node.numpy(),
        betti_0,
        betti_1,
        degrees,
        clustering
    ])

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    # åœ¨IDæ•°æ®ä¸Šè®­ç»ƒ
    id_mask = (data.ood_labels == 0).numpy()

    # ä¼ªæ ‡ç­¾ï¼šé«˜å¼‚é…æ€§â†’æ½œåœ¨OOD
    median_h = np.median(h_node[id_mask].numpy())
    train_labels = (h_node[id_mask].numpy() > median_h).astype(int)

    # é€»è¾‘å›å½’
    clf = LogisticRegression(
        max_iter=1000,
        random_state=42,
        class_weight='balanced'
    )
    clf.fit(features_scaled[id_mask], train_labels)

    # é¢„æµ‹OODåˆ†æ•°
    ood_scores = clf.predict_proba(features_scaled)[:, 1]

    return ood_scores, clf, scaler

def compute_fpr95(y_true, y_scores):
    """è®¡ç®—FPR@95% TPR"""
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    idx = np.argmin(np.abs(tpr - 0.95))
    return fpr[idx]

# ==================== å®Œæ•´å®éªŒ ====================

def run_c4tda_transformation_experiment(datasets=['CLINC150']):
    """
    å®Œæ•´çš„C4-TDAè½¬åŒ–éªŒè¯å®éªŒ
    """
    print("="*80)
    print("C4-TDAå‡è®¾åˆ°åº”ç”¨çš„è½¬åŒ–éªŒè¯å®éªŒ")
    print("="*80)

    all_results = {}

    for dataset_name in datasets:
        print(f"\n{'='*80}")
        print(f"æ•°æ®é›†: {dataset_name}")
        print(f"{'='*80}")

        # åŠ è½½æ•°æ®
        if dataset_name == 'CLINC150':
            data = load_clinc150_for_ood()
        # å¯ä»¥æ·»åŠ å…¶ä»–æ•°æ®é›†

        y_true = data.ood_labels.numpy()

        # è®¡ç®—æ‹“æ‰‘ç‰¹å¾
        print("\næ­¥éª¤1: è®¡ç®—æ‹“æ‰‘ç‰¹å¾")
        h_node = compute_heterophily_pseudo(data)
        betti_0, betti_1 = compute_node_betti_numbers(data)

        print(f"\næ‹“æ‰‘ç‰¹å¾ç»Ÿè®¡:")
        print(f"  å¼‚é…æ€§: {h_node.mean():.4f} Â± {h_node.std():.4f}")
        print(f"  Î²â‚€: {betti_0.mean():.4f} Â± {betti_0.std():.4f}")
        print(f"  Î²â‚: {betti_1.mean():.4f} Â± {betti_1.std():.4f}")

        # ID vs OODç»Ÿè®¡
        id_mask = (y_true == 0)
        ood_mask = (y_true == 1)

        print(f"\nID vs OODç‰¹å¾å¯¹æ¯”:")
        print(f"  IDå¼‚é…æ€§:  {h_node[id_mask].mean():.4f} Â± {h_node[id_mask].std():.4f}")
        print(f"  OODå¼‚é…æ€§: {h_node[ood_mask].mean():.4f} Â± {h_node[ood_mask].std():.4f}")
        print(f"  ID Î²â‚:    {betti_1[id_mask].mean():.4f} Â± {betti_1[id_mask].std():.4f}")
        print(f"  OOD Î²â‚:   {betti_1[ood_mask].mean():.4f} Â± {betti_1[ood_mask].std():.4f}")

        # æ–¹æ³•1: Î²â‚ç›´æ¥
        print("\næ­¥éª¤2: æ–¹æ³•1 - Î²â‚ç›´æ¥ä½œä¸ºOODåˆ†æ•°")
        ood_scores_beta1 = method1_direct_beta1(betti_1)
        auroc_beta1 = roc_auc_score(y_true, ood_scores_beta1)
        fpr95_beta1 = compute_fpr95(y_true, ood_scores_beta1)
        aupr_beta1 = average_precision_score(y_true, ood_scores_beta1)

        print(f"  AUROC: {auroc_beta1:.4f}")
        print(f"  FPR95: {fpr95_beta1:.4f}")
        print(f"  AUPR:  {aupr_beta1:.4f}")

        # æ–¹æ³•2: h*Î²â‚
        print("\næ­¥éª¤3: æ–¹æ³•2 - h*Î²â‚ï¼ˆç»“åˆå¼‚é…æ€§ï¼‰")
        ood_scores_h_beta1 = method2_h_times_beta1(h_node, betti_1)
        auroc_h_beta1 = roc_auc_score(y_true, ood_scores_h_beta1)
        fpr95_h_beta1 = compute_fpr95(y_true, ood_scores_h_beta1)
        aupr_h_beta1 = average_precision_score(y_true, ood_scores_h_beta1)

        print(f"  AUROC: {auroc_h_beta1:.4f}")
        print(f"  FPR95: {fpr95_h_beta1:.4f}")
        print(f"  AUPR:  {aupr_h_beta1:.4f}")
        print(f"  æå‡: {(auroc_h_beta1 - auroc_beta1):.4f} ({(auroc_h_beta1/auroc_beta1-1)*100:+.1f}%)")

        # æ–¹æ³•3: è½»é‡æ ¡å‡†
        print("\næ­¥éª¤4: æ–¹æ³•3 - è½»é‡æ ¡å‡†ï¼ˆé€»è¾‘å›å½’ï¼‰")
        ood_scores_calibrated, clf, scaler = method3_calibrated(data, h_node, betti_0, betti_1)
        auroc_calibrated = roc_auc_score(y_true, ood_scores_calibrated)
        fpr95_calibrated = compute_fpr95(y_true, ood_scores_calibrated)
        aupr_calibrated = average_precision_score(y_true, ood_scores_calibrated)

        print(f"\n  AUROC: {auroc_calibrated:.4f}")
        print(f"  FPR95: {fpr95_calibrated:.4f}")
        print(f"  AUPR:  {aupr_calibrated:.4f}")
        print(f"  vs Î²â‚: {(auroc_calibrated - auroc_beta1):.4f} ({(auroc_calibrated/auroc_beta1-1)*100:+.1f}%)")
        print(f"  vs h*Î²â‚: {(auroc_calibrated - auroc_h_beta1):.4f} ({(auroc_calibrated/auroc_h_beta1-1)*100:+.1f}%)")

        # ç‰¹å¾é‡è¦æ€§
        print("\n  ç‰¹å¾é‡è¦æ€§:")
        feature_names = ['å¼‚é…æ€§', 'Î²â‚€', 'Î²â‚', 'åº¦æ•°', 'èšç±»ç³»æ•°']
        importances = np.abs(clf.coef_[0])
        sorted_idx = np.argsort(importances)[::-1]
        for i, idx in enumerate(sorted_idx):
            print(f"    {i+1}. {feature_names[idx]}: {importances[idx]:.4f}")

        # å­˜å‚¨ç»“æœ
        all_results[dataset_name] = {
            'beta1': {
                'auroc': auroc_beta1,
                'fpr95': fpr95_beta1,
                'aupr': aupr_beta1
            },
            'h_beta1': {
                'auroc': auroc_h_beta1,
                'fpr95': fpr95_h_beta1,
                'aupr': aupr_h_beta1
            },
            'calibrated': {
                'auroc': auroc_calibrated,
                'fpr95': fpr95_calibrated,
                'aupr': aupr_calibrated
            },
            'h_node': h_node,
            'betti_0': betti_0,
            'betti_1': betti_1,
            'ood_scores': {
                'beta1': ood_scores_beta1,
                'h_beta1': ood_scores_h_beta1,
                'calibrated': ood_scores_calibrated
            },
            'y_true': y_true,
            'feature_importances': dict(zip(feature_names, importances))
        }

    return all_results

# ==================== ä¸HMCENå¯¹æ¯” ====================

def compare_with_hmcen(c4tda_results, hmcen_auroc=0.8207):
    """
    ä¸HMCENç»“æœå¯¹æ¯”
    """
    print("\n" + "="*80)
    print("C4-TDA vs HMCEN æ€§èƒ½å¯¹æ¯”")
    print("="*80)

    for dataset_name, results in c4tda_results.items():
        print(f"\næ•°æ®é›†: {dataset_name}")
        print("-"*80)

        print(f"\n{'æ–¹æ³•':<30} {'AUROC':<12} {'vs HMCEN':<15} {'åˆ¤æ–­':<20}")
        print("-"*80)

        methods = [
            ('C4-TDA (Î²â‚ç›´æ¥)', results['beta1']['auroc']),
            ('C4-TDA (h*Î²â‚)', results['h_beta1']['auroc']),
            ('C4-TDA (æ ¡å‡†)', results['calibrated']['auroc']),
            ('HMCEN-C', hmcen_auroc)
        ]

        for method_name, auroc in methods:
            gap = auroc - hmcen_auroc
            gap_pct = gap / hmcen_auroc * 100

            if method_name == 'HMCEN-C':
                judgment = "åŸºå‡†"
            elif abs(gap) < 0.02:
                judgment = "æ€§èƒ½ç›¸å½“ âœ…"
            elif gap < -0.05:
                judgment = "æ˜¾è‘—è½å âŒ"
            elif gap < 0:
                judgment = "ç•¥æœ‰å·®è· âš ï¸"
            else:
                judgment = "è¶…è¶ŠHMCEN â­"

            print(f"{method_name:<30} {auroc:.4f}      {gap:+.4f} ({gap_pct:+.1f}%)   {judgment:<20}")

    # å…³é”®ç»“è®º
    print("\n" + "="*80)
    print("å…³é”®ç»“è®º:")
    print("="*80)

    best_c4tda_auroc = max(
        results['beta1']['auroc'],
        results['h_beta1']['auroc'],
        results['calibrated']['auroc']
    )

    gap = best_c4tda_auroc - hmcen_auroc

    print(f"\nC4-TDAæœ€ä½³æ–¹æ³• AUROC: {best_c4tda_auroc:.4f}")
    print(f"HMCEN-C AUROC: {hmcen_auroc:.4f}")
    print(f"å·®è·: {gap:.4f} ({gap/hmcen_auroc*100:+.1f}%)")

    print(f"\nğŸ’¡ å†³ç­–å»ºè®®:")
    if abs(gap) < 0.02:
        print("  âš ï¸ C4-TDAæ ¡å‡†åä¸HMCENæ€§èƒ½ç›¸å½“ï¼ˆÂ±2%ï¼‰")
        print("  â†’ æ¨è: ä¼˜å…ˆC4-TDA")
        print("  â†’ ç†ç”±: æ—¶é—´æ›´çŸ­ï¼ˆ3-4æœˆ vs 10-12æœˆï¼‰+ ç†è®ºå‡è®¾å·²éªŒè¯(d=0.9378)")
    elif gap < -0.05:
        print("  âŒ C4-TDAæ˜¾è‘—è½åHMCENï¼ˆ>5%ï¼‰")
        print("  â†’ æ¨è: è€ƒè™‘HMCEN-Liteï¼ˆå¦‚æœæ—¶é—´å…è®¸ï¼‰")
        print("  â†’ æ³¨æ„: éœ€è¦Prompt 4ç¡®è®¤HMCENä¼˜åŠ¿æ¥æºï¼ˆæ¶æ„ vs è®­ç»ƒåˆ†ç±»å™¨ï¼‰")
    elif gap < 0:
        print("  âš ï¸ C4-TDAç•¥é€ŠäºHMCENï¼ˆ2-5%å·®è·ï¼‰")
        print("  â†’ æ¨è: æƒè¡¡æ€§èƒ½æå‡ vs æ—¶é—´æˆæœ¬")
        print("  â†’ å»ºè®®: åŒè½¨å¹¶è¡Œï¼ˆä¸»æ¨C4-TDAï¼Œæ¢ç´¢HMCEN-Liteï¼‰")
    else:
        print("  â­ C4-TDAè¶…è¶ŠHMCEN")
        print("  â†’ æ¨è: å…¨åŠ›C4-TDAä¸»è½¨")
        print("  â†’ ä¼˜åŠ¿: æ€§èƒ½ä¼˜ + ç†è®ºä¸¥æ ¼ + æ—¶é—´çŸ­")

    return gap, best_c4tda_auroc

# ==================== å¯è§†åŒ– ====================

def visualize_transformation_results(results):
    """å¯è§†åŒ–C4-TDAè½¬åŒ–ç»“æœ"""
    # Use non-interactive backend for saving
    import matplotlib
    matplotlib.use('Agg')

    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    dataset_name = list(results.keys())[0]
    res = results[dataset_name]

    # å›¾1: AUROCå¯¹æ¯”
    methods = ['Î²â‚ç›´æ¥', 'h*Î²â‚', 'æ ¡å‡†', 'HMCEN-C']
    aurocs = [
        res['beta1']['auroc'],
        res['h_beta1']['auroc'],
        res['calibrated']['auroc'],
        0.8207  # HMCENç»“æœ
    ]
    colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']

    axes[0, 0].bar(methods, aurocs, color=colors, alpha=0.7)
    axes[0, 0].axhline(y=0.7, color='gray', linestyle='--', alpha=0.5, label='Baseline 0.7')
    axes[0, 0].set_ylabel('AUROC', fontsize=12)
    axes[0, 0].set_title('C4-TDA Three Methods vs HMCEN Comparison', fontsize=14)
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    axes[0, 0].legend()
    axes[0, 0].set_ylim(0.4, 1.0)

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (method, auroc) in enumerate(zip(methods, aurocs)):
        axes[0, 0].text(i, auroc + 0.02, f'{auroc:.3f}', ha='center', fontsize=10)

    # å›¾2: å¼‚é…æ€§ vs Î²â‚ æ•£ç‚¹å›¾
    h_node = res['h_node'].numpy()
    betti_1 = res['betti_1']
    y_true = res['y_true']

    id_mask = (y_true == 0)
    ood_mask = (y_true == 1)

    axes[0, 1].scatter(h_node[id_mask], betti_1[id_mask],
                      alpha=0.3, s=20, c='blue', label='ID samples')
    axes[0, 1].scatter(h_node[ood_mask], betti_1[ood_mask],
                      alpha=0.3, s=20, c='red', label='OOD samples')
    axes[0, 1].set_xlabel('Heterophily h(v)', fontsize=12)
    axes[0, 1].set_ylabel('Betti number Î²â‚', fontsize=12)
    axes[0, 1].set_title('Heterophily-Betti Relationship (Original Hypothesis)', fontsize=14)
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # å›¾3: OODåˆ†æ•°åˆ†å¸ƒ - Î²â‚
    axes[1, 0].hist(res['ood_scores']['beta1'][id_mask], bins=50,
                    alpha=0.5, color='blue', label='ID (Î²â‚)', density=True)
    axes[1, 0].hist(res['ood_scores']['beta1'][ood_mask], bins=50,
                    alpha=0.5, color='red', label='OOD (Î²â‚)', density=True)
    axes[1, 0].set_xlabel('OOD Score', fontsize=12)
    axes[1, 0].set_ylabel('Density', fontsize=12)
    axes[1, 0].set_title('Î²â‚ Direct as OOD Score Distribution', fontsize=14)
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # å›¾4: æ ¡å‡†åçš„OODåˆ†æ•°åˆ†å¸ƒ
    axes[1, 1].hist(res['ood_scores']['calibrated'][id_mask], bins=50,
                    alpha=0.5, color='blue', label='ID (Calibrated)', density=True)
    axes[1, 1].hist(res['ood_scores']['calibrated'][ood_mask], bins=50,
                    alpha=0.5, color='red', label='OOD (Calibrated)', density=True)
    axes[1, 1].set_xlabel('OOD Score', fontsize=12)
    axes[1, 1].set_ylabel('Density', fontsize=12)
    axes[1, 1].set_title('Lightweight Calibrated OOD Score Distribution', fontsize=14)
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('c4tda_transformation_analysis.png', dpi=300, bbox_inches='tight')
    print("\nå¯è§†åŒ–ç»“æœå·²ä¿å­˜: c4tda_transformation_analysis.png")

def visualize_feature_importance(results):
    """å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§"""
    import matplotlib
    matplotlib.use('Agg')

    dataset_name = list(results.keys())[0]
    res = results[dataset_name]

    fig, ax = plt.subplots(figsize=(10, 6))

    feature_names = list(res['feature_importances'].keys())
    importances = list(res['feature_importances'].values())

    # æ’åº
    sorted_idx = np.argsort(importances)[::-1]
    sorted_names = [feature_names[i] for i in sorted_idx]
    sorted_importances = [importances[i] for i in sorted_idx]

    colors = ['#e74c3c' if 'Î²' in name else '#3498db' for name in sorted_names]

    bars = ax.barh(sorted_names, sorted_importances, color=colors, alpha=0.7)
    ax.set_xlabel('Feature Importance (|coefficient|)', fontsize=12)
    ax.set_title('Logistic Regression Feature Importance for OOD Detection', fontsize=14)
    ax.grid(True, alpha=0.3, axis='x')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, imp in zip(bars, sorted_importances):
        ax.text(imp + 0.01, bar.get_y() + bar.get_height()/2,
                f'{imp:.3f}', va='center', fontsize=10)

    plt.tight_layout()
    plt.savefig('c4tda_feature_importance.png', dpi=300, bbox_inches='tight')
    print("ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: c4tda_feature_importance.png")

def generate_summary_table(results, hmcen_auroc=0.8207):
    """ç”Ÿæˆæ±‡æ€»è¡¨æ ¼"""
    print("\n" + "="*80)
    print("æ±‡æ€»è¡¨æ ¼")
    print("="*80)

    dataset_name = list(results.keys())[0]
    res = results[dataset_name]

    print(f"\n{'æ–¹æ³•':<25} {'AUROC':<10} {'FPR95':<10} {'AUPR':<10} {'åˆ¤æ–­':<20}")
    print("-"*75)

    methods_data = [
        ('C4-TDA (Î²â‚ç›´æ¥)', res['beta1']),
        ('C4-TDA (h*Î²â‚)', res['h_beta1']),
        ('C4-TDA (æ ¡å‡†)', res['calibrated']),
    ]

    for name, data in methods_data:
        gap = data['auroc'] - hmcen_auroc
        if abs(gap) < 0.02:
            judgment = "âœ… ç›¸å½“"
        elif gap < -0.05:
            judgment = "âŒ æ˜¾è‘—è½å"
        elif gap < 0:
            judgment = "âš ï¸ ç•¥æœ‰å·®è·"
        else:
            judgment = "â­ è¶…è¶Š"

        print(f"{name:<25} {data['auroc']:.4f}     {data['fpr95']:.4f}     {data['aupr']:.4f}     {judgment}")

    print(f"{'HMCEN-C (åŸºå‡†)':<25} {hmcen_auroc:.4f}     {'N/A':<10} {'N/A':<10} {'åŸºå‡†'}")
    print("-"*75)

# ==================== åˆ¤æ–­é€»è¾‘ ====================

def make_final_decision(results, hmcen_auroc=0.8207):
    """æ ¹æ®å®éªŒç»“æœåšå‡ºæœ€ç»ˆå†³ç­–"""
    print("\n" + "="*80)
    print("æœ€ç»ˆå†³ç­–åˆ†æ")
    print("="*80)

    dataset_name = list(results.keys())[0]
    res = results[dataset_name]

    beta1_auroc = res['beta1']['auroc']
    calibrated_auroc = res['calibrated']['auroc']

    # åˆ¤æ–­1: ç›´æ¥åº”ç”¨èƒ½åŠ›
    print("\nåˆ¤æ–­1: ç›´æ¥åº”ç”¨èƒ½åŠ›")
    if beta1_auroc >= 0.7:
        print(f"  âœ… Î²â‚ AUROC = {beta1_auroc:.4f} â‰¥ 0.7")
        print("  â†’ å‡è®¾å¯ä»¥ç›´æ¥ç”¨äºOODæ£€æµ‹")
        direct_judgment = "å¯ç›´æ¥ä½¿ç”¨"
    elif beta1_auroc >= 0.6:
        print(f"  âš ï¸ Î²â‚ AUROC = {beta1_auroc:.4f}ï¼Œåœ¨0.6-0.7èŒƒå›´")
        print("  â†’ éœ€è¦é¢å¤–æ ¡å‡†")
        direct_judgment = "éœ€è¦æ ¡å‡†"
    else:
        print(f"  âŒ Î²â‚ AUROC = {beta1_auroc:.4f} < 0.6")
        print("  â†’ å‡è®¾éªŒè¯æˆåŠŸï¼Œä½†è½¬åŒ–å›°éš¾")
        direct_judgment = "è½¬åŒ–å›°éš¾"

    # åˆ¤æ–­2: æ ¡å‡†åä¸HMCENå¯¹æ¯”
    print("\nåˆ¤æ–­2: æ ¡å‡†åä¸HMCENå¯¹æ¯”")
    gap = calibrated_auroc - hmcen_auroc
    gap_pct = abs(gap / hmcen_auroc * 100)

    if abs(gap) <= 0.02:
        print(f"  âœ… å·®è· = {gap:.4f} ({gap_pct:.1f}%) â‰¤ 2%")
        print("  â†’ æ€§èƒ½ç›¸å½“ï¼Œä¼˜å…ˆC4-TDAï¼ˆæ—¶é—´çŸ­ï¼‰")
        comparison_judgment = "æ€§èƒ½ç›¸å½“"
    elif gap < -0.05:
        print(f"  âŒ å·®è· = {gap:.4f} ({gap_pct:.1f}%) > 5%")
        print("  â†’ HMCENæœ‰å®è´¨æ€§ä¼˜åŠ¿")
        comparison_judgment = "HMCENæ›´ä¼˜"
    elif gap < 0:
        print(f"  âš ï¸ å·®è· = {gap:.4f} ({gap_pct:.1f}%)ï¼Œåœ¨2-5%èŒƒå›´")
        print("  â†’ éœ€è¦æƒè¡¡æ€§èƒ½ vs æ—¶é—´")
        comparison_judgment = "ç•¥é€Šä¸€ç­¹"
    else:
        print(f"  â­ C4-TDAè¶…è¶ŠHMCEN! å·®è· = {gap:.4f} ({gap_pct:.1f}%)")
        comparison_judgment = "C4-TDAæ›´ä¼˜"

    # åˆ¤æ–­3: ç‰¹å¾é‡è¦æ€§
    print("\nåˆ¤æ–­3: ç‰¹å¾é‡è¦æ€§åˆ†æ")
    beta1_importance = res['feature_importances']['Î²â‚']
    total_importance = sum(res['feature_importances'].values())
    beta1_ratio = beta1_importance / total_importance * 100

    if beta1_ratio > 40:
        print(f"  âœ… Î²â‚é‡è¦æ€§ = {beta1_ratio:.1f}% > 40%")
        print("  â†’ æ‹“æ‰‘ç‰¹å¾ç¡®å®æœ‰ç”¨")
        feature_judgment = "æ‹“æ‰‘ç‰¹å¾æœ‰æ•ˆ"
    elif beta1_ratio > 20:
        print(f"  âš ï¸ Î²â‚é‡è¦æ€§ = {beta1_ratio:.1f}%ï¼Œåœ¨20-40%èŒƒå›´")
        print("  â†’ æ‹“æ‰‘ç‰¹å¾æœ‰ä¸€å®šè´¡çŒ®")
        feature_judgment = "æ‹“æ‰‘ç‰¹å¾æœ‰è´¡çŒ®"
    else:
        print(f"  âš ï¸ Î²â‚é‡è¦æ€§ = {beta1_ratio:.1f}% < 20%")
        print("  â†’ å…¶ä»–ç‰¹å¾æ›´é‡è¦ï¼ŒC4-TDAæ ¸å¿ƒä»·å€¼è¢«ç¨€é‡Š")
        feature_judgment = "æ ¸å¿ƒä»·å€¼ç¨€é‡Š"

    # ç»¼åˆå†³ç­–
    print("\n" + "="*80)
    print("ç»¼åˆå†³ç­–")
    print("="*80)

    print(f"\n1. ç›´æ¥åº”ç”¨èƒ½åŠ›: {direct_judgment}")
    print(f"2. ä¸HMCENå¯¹æ¯”: {comparison_judgment}")
    print(f"3. ç‰¹å¾é‡è¦æ€§: {feature_judgment}")

    # æœ€ç»ˆæ¨è
    print("\n" + "-"*40)
    print("æœ€ç»ˆæ¨è:")
    print("-"*40)

    if comparison_judgment in ["æ€§èƒ½ç›¸å½“", "C4-TDAæ›´ä¼˜"]:
        print("\nğŸ¯ æ¨è: C4-TDAä¸»è½¨")
        print("   ç†ç”±:")
        print("   - æ€§èƒ½ä¸HMCENç›¸å½“æˆ–æ›´ä¼˜")
        print("   - æ—¶é—´æ›´çŸ­ï¼ˆ3-4æœˆ vs 10-12æœˆï¼‰")
        print("   - ç†è®ºå‡è®¾å·²éªŒè¯(d=0.9378)")
    elif comparison_judgment == "ç•¥é€Šä¸€ç­¹":
        print("\nğŸ¯ æ¨è: åŒè½¨å¹¶è¡Œ")
        print("   - ä¸»æ¨C4-TDAï¼ˆç†è®ºå®Œå¤‡+æ—¶é—´çŸ­ï¼‰")
        print("   - æ¢ç´¢HMCEN-Liteï¼ˆå¦‚æœæ—¶é—´å…è®¸ï¼‰")
        print("   - å…³æ³¨Prompt 4ç»“æœï¼ˆHMCENä¼˜åŠ¿æ¥æºåˆ†æï¼‰")
    else:
        print("\nğŸ¯ æ¨è: é‡æ–°è¯„ä¼°æ–¹æ¡ˆ")
        print("   - HMCENæœ‰æ˜æ˜¾æ€§èƒ½ä¼˜åŠ¿")
        print("   - è€ƒè™‘HMCEN-Liteæ–¹æ¡ˆ")
        print("   - æˆ–æ¥å—æ€§èƒ½å·®è·ï¼ŒåšæŒC4-TDAç†è®ºè·¯çº¿")

    return {
        'direct_judgment': direct_judgment,
        'comparison_judgment': comparison_judgment,
        'feature_judgment': feature_judgment,
        'beta1_auroc': beta1_auroc,
        'calibrated_auroc': calibrated_auroc,
        'gap': gap
    }

# ==================== ä¸»å‡½æ•° ====================

if __name__ == '__main__':
    print("\nå¼€å§‹C4-TDAè½¬åŒ–èƒ½åŠ›éªŒè¯å®éªŒ...")
    print("="*80)

    # è¿è¡Œå®éªŒ
    results = run_c4tda_transformation_experiment(datasets=['CLINC150'])

    # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
    generate_summary_table(results)

    # ä¸HMCENå¯¹æ¯”
    gap, best_auroc = compare_with_hmcen(results, hmcen_auroc=0.8207)

    # æœ€ç»ˆå†³ç­–
    decision = make_final_decision(results)

    # å¯è§†åŒ–
    visualize_transformation_results(results)
    visualize_feature_importance(results)

    print("\n" + "="*80)
    print("å®éªŒå®Œæˆï¼")
    print("="*80)
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - c4tda_transformation_analysis.png")
    print("  - c4tda_feature_importance.png")
