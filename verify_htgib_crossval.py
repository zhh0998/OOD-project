#!/usr/bin/env python3
"""
HT-GIBå‡è®¾äº¤å‰éªŒè¯
åœ¨NYT10è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šåˆ†åˆ«éªŒè¯ï¼Œç¡®è®¤ç»“æœä¸€è‡´æ€§
"""

import json
import numpy as np
from collections import defaultdict
import random
import warnings
warnings.filterwarnings('ignore')

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import scipy.stats as stats
import networkx as nx

print("=" * 70)
print("HT-GIB å‡è®¾äº¤å‰éªŒè¯")
print("éªŒè¯: å¼‚é…æ€§ h(v) ä¸ å™ªå£°ç‡ N(v) çš„ç›¸å…³æ€§")
print("=" * 70)

def validate_hypothesis(data_path, dataset_name, sample_size=15000):
    """åœ¨æŒ‡å®šæ•°æ®é›†ä¸ŠéªŒè¯HT-GIBå‡è®¾"""

    print(f"\n{'='*70}")
    print(f"æ•°æ®é›†: {dataset_name}")
    print(f"{'='*70}")

    # A. åŠ è½½æ•°æ®
    print(f"\n[A] åŠ è½½æ•°æ®...")
    random.seed(42)

    all_data = []
    with open(data_path, 'r') as f:
        for line in f:
            try:
                item = json.loads(line.strip())
                all_data.append(item)
            except:
                continue

    print(f"    æ€»æ ·æœ¬æ•°: {len(all_data)}")

    # éšæœºé‡‡æ ·
    data = random.sample(all_data, min(sample_size, len(all_data)))
    print(f"    é‡‡æ ·æ•°: {len(data)}")

    # ç»Ÿè®¡NAæ¯”ä¾‹
    na_count = sum(1 for item in data if item['relation'] == 'NA')
    print(f"    NAæ ·æœ¬: {na_count} ({100*na_count/len(data):.1f}%)")

    # B. æ„å»ºå›¾
    print(f"\n[B] æ„å»ºå®ä½“å…±ç°å›¾...")
    G = nx.Graph()
    entity_sentences = defaultdict(list)

    for item in data:
        head = item['h']['name']
        tail = item['t']['name']
        text = item['text']

        G.add_node(head)
        G.add_node(tail)
        if head != tail:
            G.add_edge(head, tail)

        entity_sentences[head].append(text)
        entity_sentences[tail].append(text)

    print(f"    èŠ‚ç‚¹: {G.number_of_nodes()}, è¾¹: {G.number_of_edges()}")

    # C. è®¡ç®—å®ä½“åµŒå…¥
    print(f"\n[C] è®¡ç®—å®ä½“åµŒå…¥...")
    model = SentenceTransformer('all-MiniLM-L6-v2')

    entity_embeddings = {}
    entities_list = list(entity_sentences.keys())

    for idx, entity in enumerate(entities_list):
        sentences = entity_sentences[entity]
        if len(sentences) > 0:
            embs = model.encode(sentences[:5], show_progress_bar=False)
            entity_embeddings[entity] = np.mean(embs, axis=0)

        if (idx + 1) % 1000 == 0:
            print(f"    å·²å¤„ç† {idx + 1}/{len(entities_list)}")

    print(f"    åµŒå…¥æ•°: {len(entity_embeddings)}")

    # D. è®¡ç®—å¼‚é…æ€§
    print(f"\n[D] è®¡ç®—å¼‚é…æ€§...")
    heterophily_scores = {}

    for node in G.nodes():
        if node not in entity_embeddings:
            continue

        neighbors = list(G.neighbors(node))
        if len(neighbors) == 0:
            continue

        neighbor_embs = [entity_embeddings[n] for n in neighbors if n in entity_embeddings]
        if len(neighbor_embs) == 0:
            continue

        node_emb = entity_embeddings[node].reshape(1, -1)
        neighbor_matrix = np.array(neighbor_embs)
        similarities = cosine_similarity(node_emb, neighbor_matrix)[0]

        h_v = 1.0 - np.mean(similarities)
        heterophily_scores[node] = h_v

    print(f"    å¼‚é…æ€§èŠ‚ç‚¹æ•°: {len(heterophily_scores)}")

    # E. è®¡ç®—å™ªå£°ç‡
    print(f"\n[E] è®¡ç®—å™ªå£°ç‡...")
    entity_noise_stats = defaultdict(lambda: {'total': 0, 'noise': 0})

    for item in data:
        head = item['h']['name']
        tail = item['t']['name']
        relation = item['relation']

        entity_noise_stats[head]['total'] += 1
        entity_noise_stats[tail]['total'] += 1

        if relation == 'NA':
            entity_noise_stats[head]['noise'] += 1
            entity_noise_stats[tail]['noise'] += 1

    noise_rates = {}
    for entity, st in entity_noise_stats.items():
        if st['total'] > 0:
            noise_rates[entity] = st['noise'] / st['total']

    print(f"    å™ªå£°ç‡èŠ‚ç‚¹æ•°: {len(noise_rates)}")

    # F. ç»Ÿè®¡åˆ†æ
    print(f"\n[F] ç»Ÿè®¡åˆ†æ...")
    common_entities = set(heterophily_scores.keys()) & set(noise_rates.keys())
    print(f"    åˆ†æèŠ‚ç‚¹æ•°: {len(common_entities)}")

    h_values = [heterophily_scores[e] for e in common_entities]
    n_values = [noise_rates[e] for e in common_entities]

    # Pearson
    r_pearson, p_pearson = stats.pearsonr(h_values, n_values)

    # Spearman
    r_spearman, p_spearman = stats.spearmanr(h_values, n_values)

    # Cohen's d
    h_array = np.array(h_values)
    n_array = np.array(n_values)

    q1 = np.percentile(h_array, 25)
    q4 = np.percentile(h_array, 75)

    low_noise = n_array[h_array <= q1]
    high_noise = n_array[h_array >= q4]

    mean_low = np.mean(low_noise)
    mean_high = np.mean(high_noise)
    std_low = np.std(low_noise, ddof=1)
    std_high = np.std(high_noise, ddof=1)
    n_low = len(low_noise)
    n_high = len(high_noise)

    pooled_std = np.sqrt(((n_low-1)*std_low**2 + (n_high-1)*std_high**2) / (n_low + n_high - 2))
    cohens_d = (mean_high - mean_low) / pooled_std if pooled_std > 0 else 0

    # è¾“å‡ºç»“æœ
    results = {
        'dataset': dataset_name,
        'n_samples': len(data),
        'n_entities': len(common_entities),
        'na_ratio': na_count / len(data),
        'mean_heterophily': np.mean(h_values),
        'mean_noise_rate': np.mean(n_values),
        'pearson_r': r_pearson,
        'pearson_p': p_pearson,
        'spearman_r': r_spearman,
        'spearman_p': p_spearman,
        'cohens_d': cohens_d,
        'q1_noise': mean_low,
        'q4_noise': mean_high,
        'n_q1': n_low,
        'n_q4': n_high
    }

    print(f"\n{'='*50}")
    print(f"ç»“æœ: {dataset_name}")
    print(f"{'='*50}")
    print(f"æ ·æœ¬æ•°: {results['n_samples']}")
    print(f"å®ä½“æ•°: {results['n_entities']}")
    print(f"NAæ¯”ä¾‹: {results['na_ratio']:.1%}")
    print(f"å¹³å‡å¼‚é…æ€§: {results['mean_heterophily']:.3f}")
    print(f"å¹³å‡å™ªå£°ç‡: {results['mean_noise_rate']:.3f}")
    print()
    print(f"Pearson r  = {results['pearson_r']:+.4f} (p={results['pearson_p']:.2e})")
    print(f"Spearman Ï = {results['spearman_r']:+.4f} (p={results['spearman_p']:.2e})")
    print(f"Cohen's d  = {results['cohens_d']:+.4f}  â† å…³é”®æŒ‡æ ‡")
    print()
    print(f"Q1 (ä½å¼‚é…) å™ªå£°ç‡: {results['q1_noise']:.4f} (n={results['n_q1']})")
    print(f"Q4 (é«˜å¼‚é…) å™ªå£°ç‡: {results['q4_noise']:.4f} (n={results['n_q4']})")
    print(f"å·®å¼‚: {results['q4_noise'] - results['q1_noise']:+.4f}")

    return results

# ============================================================
# ä¸»ç¨‹åºï¼šäº¤å‰éªŒè¯
# ============================================================

print("\n" + "=" * 70)
print("å¼€å§‹äº¤å‰éªŒè¯...")
print("=" * 70)

# éªŒè¯è®­ç»ƒé›†
train_results = validate_hypothesis(
    'nyt10/nyt10_train.txt',
    'NYT10-Train',
    sample_size=15000
)

# éªŒè¯æµ‹è¯•é›†
test_results = validate_hypothesis(
    'nyt10/nyt10_test.txt',
    'NYT10-Test',
    sample_size=15000
)

# ============================================================
# å¯¹æ¯”åˆ†æ
# ============================================================

print("\n" + "=" * 70)
print("äº¤å‰éªŒè¯ç»“æœå¯¹æ¯”")
print("=" * 70)

print("\næŒ‡æ ‡              NYT10-Train    NYT10-Test     ä¸€è‡´æ€§")
print("-" * 70)

# Cohen's d
d_train = train_results['cohens_d']
d_test = test_results['cohens_d']
d_consistent = "âœ…" if (d_train < -0.3 and d_test < -0.3) or (d_train > 0.3 and d_test > 0.3) else "âŒ"
print(f"Cohen's d        {d_train:+.4f}        {d_test:+.4f}        {d_consistent}")

# Pearson r
r_train = train_results['pearson_r']
r_test = test_results['pearson_r']
r_consistent = "âœ…" if (r_train < 0 and r_test < 0) or (r_train > 0 and r_test > 0) else "âŒ"
print(f"Pearson r        {r_train:+.4f}        {r_test:+.4f}        {r_consistent}")

# Spearman
s_train = train_results['spearman_r']
s_test = test_results['spearman_r']
s_consistent = "âœ…" if (s_train < 0 and s_test < 0) or (s_train > 0 and s_test > 0) else "âŒ"
print(f"Spearman Ï       {s_train:+.4f}        {s_test:+.4f}        {s_consistent}")

# Q1å™ªå£°ç‡
q1_train = train_results['q1_noise']
q1_test = test_results['q1_noise']
print(f"Q1å™ªå£°ç‡         {q1_train:.4f}         {q1_test:.4f}")

# Q4å™ªå£°ç‡
q4_train = train_results['q4_noise']
q4_test = test_results['q4_noise']
print(f"Q4å™ªå£°ç‡         {q4_train:.4f}         {q4_test:.4f}")

print("-" * 70)

# ============================================================
# æœ€ç»ˆç»“è®º
# ============================================================

print("\n" + "=" * 70)
print("æœ€ç»ˆç»“è®º")
print("=" * 70)

# åˆ¤æ–­ä¸€è‡´æ€§
both_negative = d_train < -0.3 and d_test < -0.3
both_positive = d_train > 0.3 and d_test > 0.3
both_insignificant = abs(d_train) < 0.3 and abs(d_test) < 0.3

if both_negative:
    print("\nâœ… ç»“æœé«˜åº¦ä¸€è‡´ï¼")
    print()
    print(f"è®­ç»ƒé›† Cohen's d = {d_train:+.4f} (è´Ÿç›¸å…³)")
    print(f"æµ‹è¯•é›† Cohen's d = {d_test:+.4f} (è´Ÿç›¸å…³)")
    print()
    print("ç»“è®º: HT-GIBæ ¸å¿ƒå‡è®¾åœ¨NYT10æ•°æ®é›†ä¸Šå½»åº•å¤±è´¥ï¼")
    print()
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚  å‡è®¾é¢„æœŸ: é«˜å¼‚é…æ€§ â†’ é«˜å™ªå£°ç‡ (æ­£ç›¸å…³)             â”‚")
    print("â”‚  å®é™…ç»“æœ: é«˜å¼‚é…æ€§ â†’ ä½å™ªå£°ç‡ (è´Ÿç›¸å…³)             â”‚")
    print("â”‚  æ–¹å‘å®Œå…¨ç›¸åï¼                                      â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    print()
    print("ğŸ“‹ å¼ºçƒˆå»ºè®®:")
    print("   1. ç«‹å³æ”¾å¼ƒ HT-GIB æ–¹æ¡ˆ")
    print("   2. åˆ‡æ¢åˆ°å¤‡é€‰æ–¹æ¡ˆ:")
    print("      â€¢ HDCL-RE (å¼‚æ„åŒå¡”å¯¹æ¯”å­¦ä¹ )")
    print("      â€¢ æ ‡å‡†å¯¹æ¯”å­¦ä¹ å»å™ª")
    print("      â€¢ ä¸ç¡®å®šæ€§å¼•å¯¼æ³¨æ„åŠ›æœºåˆ¶")
    print()
    print("â±ï¸ éªŒè¯æ—¶é—´: çº¦2å°æ—¶")
    print("ğŸ’° åŠæ—¶æ­¢æŸï¼Œé¿å…æµªè´¹4å‘¨å®æ–½æ—¶é—´")

elif both_positive:
    print("\nâœ… ç»“æœä¸€è‡´ï¼šå‡è®¾æˆç«‹ï¼")
    print()
    print(f"è®­ç»ƒé›† Cohen's d = {d_train:+.4f} (æ­£ç›¸å…³)")
    print(f"æµ‹è¯•é›† Cohen's d = {d_test:+.4f} (æ­£ç›¸å…³)")
    print()
    print("ç»“è®º: HT-GIBå‡è®¾å¾—åˆ°éªŒè¯")
    print("å»ºè®®: ç»§ç»­Phase 2å®æ–½")

elif both_insignificant:
    print("\nâš ï¸ ä¸¤ä¸ªæ•°æ®é›†éƒ½æ— æ˜¾è‘—ç›¸å…³")
    print()
    print(f"è®­ç»ƒé›† Cohen's d = {d_train:+.4f}")
    print(f"æµ‹è¯•é›† Cohen's d = {d_test:+.4f}")
    print()
    print("ç»“è®º: HT-GIBå‡è®¾æ— è¶³å¤Ÿè¯æ®æ”¯æŒ")
    print("å»ºè®®: è°¨æ…è€ƒè™‘æ˜¯å¦ç»§ç»­")

else:
    print("\nâŒ ç»“æœä¸ä¸€è‡´")
    print()
    print(f"è®­ç»ƒé›† Cohen's d = {d_train:+.4f}")
    print(f"æµ‹è¯•é›† Cohen's d = {d_test:+.4f}")
    print()
    print("ç»“è®º: å‡è®¾å¯èƒ½ä¸æ•°æ®åˆ†å¸ƒç›¸å…³")
    print("å»ºè®®: éœ€è¦è¿›ä¸€æ­¥åˆ†æ")

print("\n" + "=" * 70)
